{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8d4d4b",
   "metadata": {},
   "source": [
    "1. Konfigurasi API dan Inisialisasi Klien\n",
    "Ganti nilai CDS_API_KEY dengan API key Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360feecc",
   "metadata": {},
   "source": [
    "2. Fungsi Download Data ERA5 (Mengambil Data Setahun Penuh)  \n",
    "Request ini meminta semua bulan dan semua hari pada tahun yang ditentukan di Cell 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21cad1",
   "metadata": {},
   "source": [
    "3. Pemrosesan Data (Agregasi Harian & Konversi Unit)  \n",
    "Ganti YEAR_TO_PROCESS dengan tahun yang baru selesai Anda unduh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2da467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 22:06:11,706 - INFO - üöÄ MEMULAI ERA5 DATA PROCESSING PIPELINE\n",
      "2025-10-31 22:06:11,708 - INFO - üîß Setup environment...\n",
      "2025-10-31 22:06:11,709 - INFO - ‚úÖ xarray tersedia\n",
      "2025-10-31 22:06:11,710 - INFO - ‚úÖ pandas tersedia\n",
      "2025-10-31 22:06:11,711 - INFO - ‚úÖ numpy tersedia\n",
      "2025-10-31 22:06:11,713 - INFO - ‚úÖ cdsapi tersedia\n",
      "2025-10-31 22:06:11,716 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 22:06:11,719 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\processed_data\n",
      "2025-10-31 22:06:11,722 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\csv_output\n",
      "2025-10-31 22:06:11,724 - INFO - üì• Download data ERA5...\n",
      "2025-10-31 22:06:23,017 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/catalogue/v1/messages (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A64C27EE90>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 1 of 500\n",
      "2025-10-31 22:06:23,020 - WARNING - Retrying in 120 seconds\n",
      "2025-10-31 22:08:23,023 - INFO - Retrying now...\n",
      "2025-10-31 22:08:30,058 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/catalogue/v1/messages (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A64C27FD90>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11002] getaddrinfo failed)\"))], attempt 2 of 500\n",
      "2025-10-31 22:08:30,060 - WARNING - Retrying in 120 seconds\n",
      "2025-10-31 22:10:30,063 - INFO - Retrying now...\n",
      "2025-10-31 22:10:41,343 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/catalogue/v1/messages (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A64C698190>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 3 of 500\n",
      "2025-10-31 22:10:41,345 - WARNING - Retrying in 120 seconds\n",
      "2025-10-31 22:12:41,347 - INFO - Retrying now...\n",
      "2025-10-31 22:12:43,680 - INFO - CDS API client initialized\n",
      "2025-10-31 22:12:43,682 - INFO - Downloading data untuk 2022-03...\n",
      "2025-10-31 22:12:44,940 INFO Request ID is ce547519-8fb2-4236-b3f9-48dd3b73dce0\n",
      "2025-10-31 22:12:44,940 - INFO - Request ID is ce547519-8fb2-4236-b3f9-48dd3b73dce0\n",
      "2025-10-31 22:12:45,277 INFO status has been updated to accepted\n",
      "2025-10-31 22:12:45,277 - INFO - status has been updated to accepted\n",
      "2025-10-31 22:12:54,746 INFO status has been updated to running\n",
      "2025-10-31 22:12:54,746 - INFO - status has been updated to running\n",
      "2025-10-31 22:13:00,544 INFO status has been updated to successful\n",
      "2025-10-31 22:13:00,544 - INFO - status has been updated to successful\n",
      "2025-10-31 22:13:01,253 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-10-29/93e4ad9328c1d6149715a6b6c77ba23b.zip\n",
      "2025-10-31 22:13:05,532 - INFO - ‚úÖ Download selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc\n",
      "2025-10-31 22:13:05,533 - INFO - Total files downloaded: 1\n",
      "2025-10-31 22:13:05,534 - INFO - üîÑ Konversi NetCDF ke CSV...\n",
      "2025-10-31 22:13:05,536 - INFO - Menemukan 2 file untuk dikonversi:\n",
      "2025-10-31 22:13:05,537 - INFO -    1. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc\n",
      "2025-10-31 22:13:05,539 - INFO -    2. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc\n",
      "2025-10-31 22:13:05,540 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc\n",
      "2025-10-31 22:13:05,543 - INFO - Ukuran file: 0.18 MB\n",
      "2025-10-31 22:13:05,545 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc dengan engine: netcdf4\n",
      "2025-10-31 22:13:05,575 - WARNING - ‚ùå Engine netcdf4 gagal: [Errno -51] NetCDF: Unknown file format: 'C:\\\\Users\\\\user\\\\OneDrive\\\\IPB\\\\Thesis\\\\02. Development\\\\01. Data Praprocessing\\\\raw_data\\\\data_era5_hourly_2022_02.nc'\n",
      "2025-10-31 22:13:05,577 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc dengan engine: scipy\n",
      "2025-10-31 22:13:05,580 - WARNING - ‚ùå Engine scipy gagal: Error: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc is not a valid NetCDF 3 file\n",
      "            If this is a NetCDF4 file, you may need to install the\n",
      "            netcdf4 library, e.g.,\n",
      "\n",
      "            $ pip install netcdf4\n",
      "            \n",
      "2025-10-31 22:13:05,582 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc dengan engine: h5netcdf\n",
      "2025-10-31 22:13:05,587 - WARNING - ‚ùå Engine h5netcdf gagal: Unable to synchronously open file (file signature not found)\n",
      "2025-10-31 22:13:05,589 - INFO - Mencoba buka file tanpa specify engine...\n",
      "2025-10-31 22:13:05,593 - ERROR - ‚ùå Semua metode gagal: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'gini']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "2025-10-31 22:13:05,594 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc\n",
      "2025-10-31 22:13:05,595 - INFO - Ukuran file: 0.18 MB\n",
      "2025-10-31 22:13:05,595 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc dengan engine: netcdf4\n",
      "2025-10-31 22:13:05,699 - WARNING - ‚ùå Engine netcdf4 gagal: [Errno -51] NetCDF: Unknown file format: 'C:\\\\Users\\\\user\\\\OneDrive\\\\IPB\\\\Thesis\\\\02. Development\\\\01. Data Praprocessing\\\\raw_data\\\\data_era5_hourly_2022_03.nc'\n",
      "2025-10-31 22:13:05,700 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc dengan engine: scipy\n",
      "2025-10-31 22:13:05,701 - WARNING - ‚ùå Engine scipy gagal: Error: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc is not a valid NetCDF 3 file\n",
      "            If this is a NetCDF4 file, you may need to install the\n",
      "            netcdf4 library, e.g.,\n",
      "\n",
      "            $ pip install netcdf4\n",
      "            \n",
      "2025-10-31 22:13:05,702 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc dengan engine: h5netcdf\n",
      "2025-10-31 22:13:05,704 - WARNING - ‚ùå Engine h5netcdf gagal: Unable to synchronously open file (file signature not found)\n",
      "2025-10-31 22:13:05,705 - INFO - Mencoba buka file tanpa specify engine...\n",
      "2025-10-31 22:13:05,708 - ERROR - ‚ùå Semua metode gagal: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'cfgrib', 'gini']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\n",
      "https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\n",
      "https://docs.xarray.dev/en/stable/user-guide/io.html\n",
      "2025-10-31 22:13:05,709 - INFO - \n",
      "==================================================\n",
      "2025-10-31 22:13:05,709 - INFO - SUMMARY KONVERSI:\n",
      "2025-10-31 22:13:05,710 - INFO - ==================================================\n",
      "2025-10-31 22:13:05,711 - INFO - ‚úÖ Berhasil: 0 file\n",
      "2025-10-31 22:13:05,712 - INFO - ‚ùå Gagal: 2 file\n",
      "2025-10-31 22:13:05,713 - INFO - File yang gagal:\n",
      "2025-10-31 22:13:05,715 - INFO -   - C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_02.nc\n",
      "2025-10-31 22:13:05,716 - INFO -   - C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2022_03.nc\n",
      "2025-10-31 22:13:05,717 - INFO - \n",
      "üéâ PIPELINE SELESAI!\n",
      "2025-10-31 22:13:05,718 - INFO - üìÅ File CSV tersimpan di: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\csv_output\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:44:39,152 - INFO - üöÄ MEMULAI ERA5 DATA PROCESSING PIPELINE\n",
      "2025-11-06 10:44:39,155 - INFO - üîß Setup environment...\n",
      "2025-11-06 10:44:39,156 - INFO - ‚úÖ xarray tersedia\n",
      "2025-11-06 10:44:39,157 - INFO - ‚úÖ pandas tersedia\n",
      "2025-11-06 10:44:39,157 - INFO - ‚úÖ numpy tersedia\n",
      "2025-11-06 10:44:39,159 - INFO - ‚úÖ cdsapi tersedia\n",
      "2025-11-06 10:44:39,906 - INFO - ‚úÖ netcdf4 tersedia\n",
      "2025-11-06 10:44:39,911 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\raw_data\n",
      "2025-11-06 10:44:39,915 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\processed_data\n",
      "2025-11-06 10:44:39,918 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\csv_output\n",
      "2025-11-06 10:44:39,922 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\01. Data Praprocessing\\temp_extract\n",
      "2025-11-06 10:44:39,925 - INFO - üì• Download data ERA5...\n",
      "2025-11-06 10:44:42,574 - INFO - CDS API client initialized\n",
      "2025-11-06 10:44:42,583 - INFO - Downloading data untuk 2022-02...\n",
      "2025-11-06 10:44:43,385 INFO Request ID is feba4d49-1adb-4ec3-9db3-4f9448649331\n",
      "2025-11-06 10:44:43,385 - INFO - Request ID is feba4d49-1adb-4ec3-9db3-4f9448649331\n",
      "2025-11-06 10:44:43,711 INFO status has been updated to accepted\n",
      "2025-11-06 10:44:43,711 - INFO - status has been updated to accepted\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98212ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:54:35,242 - INFO - üöÄ MEMULAI ERA5 DATA PROCESSING PIPELINE\n",
      "2025-11-06 11:54:35,248 - INFO - üîß Setup environment...\n",
      "2025-11-06 11:54:35,249 - INFO - ‚úÖ xarray tersedia\n",
      "2025-11-06 11:54:35,250 - INFO - ‚úÖ pandas tersedia\n",
      "2025-11-06 11:54:35,251 - INFO - ‚úÖ numpy tersedia\n",
      "2025-11-06 11:54:35,252 - INFO - ‚úÖ cdsapi tersedia\n",
      "2025-11-06 11:54:35,253 - INFO - ‚úÖ netcdf4 tersedia\n",
      "2025-11-06 11:54:35,258 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\n",
      "2025-11-06 11:54:35,261 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\processed_data\n",
      "2025-11-06 11:54:35,263 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\n",
      "2025-11-06 11:54:35,265 - INFO - Directory created/verified: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\n",
      "2025-11-06 11:54:35,267 - INFO - üì• Download data ERA5...\n",
      "2025-11-06 11:54:37,275 - INFO - CDS API client initialized\n",
      "2025-11-06 11:54:37,279 - INFO - Downloading data untuk 2023-01...\n",
      "2025-11-06 11:54:38,073 INFO Request ID is 6ea87cd8-0542-4c8e-8a96-04324aebfd63\n",
      "2025-11-06 11:54:38,073 - INFO - Request ID is 6ea87cd8-0542-4c8e-8a96-04324aebfd63\n",
      "2025-11-06 11:54:38,343 INFO status has been updated to accepted\n",
      "2025-11-06 11:54:38,343 - INFO - status has been updated to accepted\n",
      "2025-11-06 11:54:53,388 INFO status has been updated to running\n",
      "2025-11-06 11:54:53,388 - INFO - status has been updated to running\n",
      "2025-11-06 11:59:04,165 INFO status has been updated to successful\n",
      "2025-11-06 11:59:04,165 - INFO - status has been updated to successful\n",
      "2025-11-06 11:59:04,739 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-11-06/d2ded33aba7c760238f299af92750536.zip\n",
      "2025-11-06 11:59:10,867 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc.temp adalah file ZIP\n",
      "2025-11-06 11:59:10,869 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc.temp\n",
      "2025-11-06 11:59:10,878 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 11:59:10,895 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 11:59:10,906 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc\n",
      "2025-11-06 11:59:10,911 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc.temp\n",
      "2025-11-06 11:59:10,915 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc\n",
      "2025-11-06 11:59:10,924 - INFO - Downloading data untuk 2023-02...\n",
      "2025-11-06 11:59:11,680 INFO Request ID is 7535ffc4-377b-42c1-9186-9a1fbeddfbe6\n",
      "2025-11-06 11:59:11,680 - INFO - Request ID is 7535ffc4-377b-42c1-9186-9a1fbeddfbe6\n",
      "2025-11-06 11:59:12,003 INFO status has been updated to accepted\n",
      "2025-11-06 11:59:12,003 - INFO - status has been updated to accepted\n",
      "2025-11-06 11:59:26,607 INFO status has been updated to running\n",
      "2025-11-06 11:59:26,607 - INFO - status has been updated to running\n",
      "2025-11-06 12:03:38,382 INFO status has been updated to successful\n",
      "2025-11-06 12:03:38,382 - INFO - status has been updated to successful\n",
      "2025-11-06 12:03:39,362 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-3/2025-11-06/3887f6ec7df083f522aa15bfc86990d1.zip\n",
      "2025-11-06 12:03:44,953 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc.temp adalah file ZIP\n",
      "2025-11-06 12:03:44,959 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc.temp\n",
      "2025-11-06 12:03:44,963 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 12:03:44,977 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 12:03:44,985 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc\n",
      "2025-11-06 12:03:44,992 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc.temp\n",
      "2025-11-06 12:03:44,999 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc\n",
      "2025-11-06 12:03:45,012 - INFO - Downloading data untuk 2023-0304...\n",
      "2025-11-06 12:03:45,727 - ERROR - ‚ùå Gagal download 2023-0304: 400 Client Error: Bad Request for url: https://cds.climate.copernicus.eu/api/retrieve/v1/processes/reanalysis-era5-single-levels/execution\n",
      "invalid request\n",
      "No valid dates from year=[2023] month=[304] day=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "2025-11-06 12:03:45,730 - INFO - Downloading data untuk 2023-05...\n",
      "2025-11-06 12:03:46,505 INFO Request ID is 1ecd088b-770b-4a4a-9faf-87b7ace1975f\n",
      "2025-11-06 12:03:46,505 - INFO - Request ID is 1ecd088b-770b-4a4a-9faf-87b7ace1975f\n",
      "2025-11-06 12:03:46,875 INFO status has been updated to accepted\n",
      "2025-11-06 12:03:46,875 - INFO - status has been updated to accepted\n",
      "2025-11-06 12:03:56,096 INFO status has been updated to running\n",
      "2025-11-06 12:03:56,096 - INFO - status has been updated to running\n",
      "2025-11-06 12:08:11,794 INFO status has been updated to successful\n",
      "2025-11-06 12:08:11,794 - INFO - status has been updated to successful\n",
      "2025-11-06 12:08:12,370 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-3/2025-11-06/d4813f76a7d701f7ed1800cb6292af50.zip\n",
      "2025-11-06 12:08:17,123 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc.temp adalah file ZIP\n",
      "2025-11-06 12:08:17,124 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc.temp\n",
      "2025-11-06 12:08:17,128 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 12:08:17,136 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 12:08:17,143 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc\n",
      "2025-11-06 12:08:17,146 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc.temp\n",
      "2025-11-06 12:08:17,149 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc\n",
      "2025-11-06 12:08:17,151 - INFO - Downloading data untuk 2023-06...\n",
      "2025-11-06 12:08:18,322 INFO Request ID is c67089ac-500a-47b4-966d-a0f8f2284a12\n",
      "2025-11-06 12:08:18,322 - INFO - Request ID is c67089ac-500a-47b4-966d-a0f8f2284a12\n",
      "2025-11-06 12:08:18,613 INFO status has been updated to accepted\n",
      "2025-11-06 12:08:18,613 - INFO - status has been updated to accepted\n",
      "2025-11-06 12:08:28,269 INFO status has been updated to running\n",
      "2025-11-06 12:08:28,269 - INFO - status has been updated to running\n",
      "2025-11-06 12:22:10,020 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/c67089ac-500a-47b4-966d-a0f8f2284a12?log=True&request=True (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E09FC147D0>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 1 of 500\n",
      "2025-11-06 12:22:10,024 - WARNING - Retrying in 120 seconds\n",
      "2025-11-06 12:24:10,028 - INFO - Retrying now...\n",
      "2025-11-06 12:24:12,177 INFO status has been updated to successful\n",
      "2025-11-06 12:24:12,177 - INFO - status has been updated to successful\n",
      "2025-11-06 12:24:12,893 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-1/2025-11-06/8d46a8f57a16ed921130aabb33d4e912.zip\n",
      "2025-11-06 12:24:17,883 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc.temp adalah file ZIP\n",
      "2025-11-06 12:24:17,885 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc.temp\n",
      "2025-11-06 12:24:17,888 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 12:24:17,894 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 12:24:17,899 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc\n",
      "2025-11-06 12:24:17,901 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc.temp\n",
      "2025-11-06 12:24:17,902 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc\n",
      "2025-11-06 12:24:17,905 - INFO - Downloading data untuk 2023-07...\n",
      "2025-11-06 12:24:18,955 INFO Request ID is 4b3f9527-786e-4965-a755-b0ad2206a724\n",
      "2025-11-06 12:24:18,955 - INFO - Request ID is 4b3f9527-786e-4965-a755-b0ad2206a724\n",
      "2025-11-06 12:24:19,342 INFO status has been updated to accepted\n",
      "2025-11-06 12:24:19,342 - INFO - status has been updated to accepted\n",
      "2025-11-06 12:24:34,084 INFO status has been updated to running\n",
      "2025-11-06 12:24:34,084 - INFO - status has been updated to running\n",
      "2025-11-06 13:20:55,212 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/4b3f9527-786e-4965-a755-b0ad2206a724?log=True&request=True (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E09FC14CD0>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 1 of 500\n",
      "2025-11-06 13:20:55,238 - WARNING - Retrying in 120 seconds\n",
      "2025-11-06 14:41:20,756 - INFO - Retrying now...\n",
      "2025-11-06 14:41:20,803 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/4b3f9527-786e-4965-a755-b0ad2206a724?log=True&request=True (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E09FC15090>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 2 of 500\n",
      "2025-11-06 14:41:20,809 - WARNING - Retrying in 120 seconds\n",
      "2025-11-06 22:55:55,627 - INFO - Retrying now...\n",
      "2025-11-06 22:55:55,871 - WARNING - Recovering from connection error [HTTPSConnectionPool(host='cds.climate.copernicus.eu', port=443): Max retries exceeded with url: /api/retrieve/v1/jobs/4b3f9527-786e-4965-a755-b0ad2206a724?log=True&request=True (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001E09FC15450>: Failed to resolve 'cds.climate.copernicus.eu' ([Errno 11001] getaddrinfo failed)\"))], attempt 3 of 500\n",
      "2025-11-06 22:55:55,876 - WARNING - Retrying in 120 seconds\n",
      "2025-11-06 22:57:55,915 - INFO - Retrying now...\n",
      "2025-11-06 22:57:58,508 INFO status has been updated to successful\n",
      "2025-11-06 22:57:58,508 - INFO - status has been updated to successful\n",
      "2025-11-06 22:57:58,983 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-3/2025-11-06/71d83bdbbe6fc197ca8df88a3db0b931.zip\n",
      "2025-11-06 22:58:04,575 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc.temp adalah file ZIP\n",
      "2025-11-06 22:58:04,577 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc.temp\n",
      "2025-11-06 22:58:04,582 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 22:58:04,589 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 22:58:04,594 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc\n",
      "2025-11-06 22:58:04,599 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc.temp\n",
      "2025-11-06 22:58:04,601 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc\n",
      "2025-11-06 22:58:04,603 - INFO - Downloading data untuk 2023-08...\n",
      "2025-11-06 22:58:05,590 INFO Request ID is 94bd451a-9864-47b2-8ce0-b5252c971b74\n",
      "2025-11-06 22:58:05,590 - INFO - Request ID is 94bd451a-9864-47b2-8ce0-b5252c971b74\n",
      "2025-11-06 22:58:05,880 INFO status has been updated to accepted\n",
      "2025-11-06 22:58:05,880 - INFO - status has been updated to accepted\n",
      "2025-11-06 22:58:15,318 INFO status has been updated to running\n",
      "2025-11-06 22:58:15,318 - INFO - status has been updated to running\n",
      "2025-11-06 23:02:30,297 INFO status has been updated to successful\n",
      "2025-11-06 23:02:30,297 - INFO - status has been updated to successful\n",
      "2025-11-06 23:02:30,812 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-1/2025-11-06/fbd547fab53051fb336e30e83f61f9de.zip\n",
      "2025-11-06 23:02:35,637 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc.temp adalah file ZIP\n",
      "2025-11-06 23:02:35,641 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc.temp\n",
      "2025-11-06 23:02:35,650 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 23:02:35,671 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 23:02:35,682 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc\n",
      "2025-11-06 23:02:35,692 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc.temp\n",
      "2025-11-06 23:02:35,698 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc\n",
      "2025-11-06 23:02:35,702 - INFO - Downloading data untuk 2023-09...\n",
      "2025-11-06 23:02:36,433 INFO Request ID is 05774291-438a-4437-ad2b-68f1986fdeb8\n",
      "2025-11-06 23:02:36,433 - INFO - Request ID is 05774291-438a-4437-ad2b-68f1986fdeb8\n",
      "2025-11-06 23:02:36,671 INFO status has been updated to accepted\n",
      "2025-11-06 23:02:36,671 - INFO - status has been updated to accepted\n",
      "2025-11-06 23:02:45,758 INFO status has been updated to running\n",
      "2025-11-06 23:02:45,758 - INFO - status has been updated to running\n",
      "2025-11-06 23:02:59,418 INFO status has been updated to accepted\n",
      "2025-11-06 23:02:59,418 - INFO - status has been updated to accepted\n",
      "2025-11-06 23:03:11,032 INFO status has been updated to running\n",
      "2025-11-06 23:03:11,032 - INFO - status has been updated to running\n",
      "2025-11-06 23:07:01,341 INFO status has been updated to successful\n",
      "2025-11-06 23:07:01,341 - INFO - status has been updated to successful\n",
      "2025-11-06 23:07:01,867 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-3/2025-11-06/b943c17bed485af801c51d6506097501.zip\n",
      "2025-11-06 23:07:07,261 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc.temp adalah file ZIP\n",
      "2025-11-06 23:07:07,264 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc.temp\n",
      "2025-11-06 23:07:07,267 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 23:07:07,278 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 23:07:07,283 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc\n",
      "2025-11-06 23:07:07,287 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc.temp\n",
      "2025-11-06 23:07:07,291 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc\n",
      "2025-11-06 23:07:07,295 - INFO - Downloading data untuk 2023-10...\n",
      "2025-11-06 23:07:08,262 INFO Request ID is 1174f53f-0d65-45d6-b813-4ec1a87f322c\n",
      "2025-11-06 23:07:08,262 - INFO - Request ID is 1174f53f-0d65-45d6-b813-4ec1a87f322c\n",
      "2025-11-06 23:07:08,516 INFO status has been updated to accepted\n",
      "2025-11-06 23:07:08,516 - INFO - status has been updated to accepted\n",
      "2025-11-06 23:07:17,948 INFO status has been updated to running\n",
      "2025-11-06 23:07:17,948 - INFO - status has been updated to running\n",
      "2025-11-06 23:11:32,852 INFO status has been updated to successful\n",
      "2025-11-06 23:11:32,852 - INFO - status has been updated to successful\n",
      "2025-11-06 23:11:33,299 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-1/2025-11-06/5b4c76f3417f76649ca606be856d0f9f.zip\n",
      "2025-11-06 23:11:37,251 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc.temp adalah file ZIP\n",
      "2025-11-06 23:11:37,253 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc.temp\n",
      "2025-11-06 23:11:37,257 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 23:11:37,267 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 23:11:37,274 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc\n",
      "2025-11-06 23:11:37,277 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc.temp\n",
      "2025-11-06 23:11:37,279 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc\n",
      "2025-11-06 23:11:37,281 - INFO - Downloading data untuk 2023-11...\n",
      "2025-11-06 23:11:38,035 INFO Request ID is 8849e137-0837-456f-bdaa-ceb08bb5899e\n",
      "2025-11-06 23:11:38,035 - INFO - Request ID is 8849e137-0837-456f-bdaa-ceb08bb5899e\n",
      "2025-11-06 23:11:38,305 INFO status has been updated to accepted\n",
      "2025-11-06 23:11:38,305 - INFO - status has been updated to accepted\n",
      "2025-11-06 23:12:12,081 INFO status has been updated to running\n",
      "2025-11-06 23:12:12,081 - INFO - status has been updated to running\n",
      "2025-11-06 23:14:33,646 INFO status has been updated to successful\n",
      "2025-11-06 23:14:33,646 - INFO - status has been updated to successful\n",
      "2025-11-06 23:14:34,544 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-11-06/ac640731f82299b636163b116a1f922e.zip\n",
      "2025-11-06 23:14:38,395 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc.temp adalah file ZIP\n",
      "2025-11-06 23:14:38,398 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc.temp\n",
      "2025-11-06 23:14:38,400 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 23:14:38,409 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 23:14:38,414 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc\n",
      "2025-11-06 23:14:38,417 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc.temp\n",
      "2025-11-06 23:14:38,419 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc\n",
      "2025-11-06 23:14:38,422 - INFO - Downloading data untuk 2023-12...\n",
      "2025-11-06 23:14:39,224 INFO Request ID is 9b072613-b709-48d1-b2d1-c72da10b882e\n",
      "2025-11-06 23:14:39,224 - INFO - Request ID is 9b072613-b709-48d1-b2d1-c72da10b882e\n",
      "2025-11-06 23:14:39,441 INFO status has been updated to accepted\n",
      "2025-11-06 23:14:39,441 - INFO - status has been updated to accepted\n",
      "2025-11-06 23:14:48,640 INFO status has been updated to running\n",
      "2025-11-06 23:14:48,640 - INFO - status has been updated to running\n",
      "2025-11-06 23:19:04,285 INFO status has been updated to successful\n",
      "2025-11-06 23:19:04,285 - INFO - status has been updated to successful\n",
      "2025-11-06 23:19:04,699 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-11-06/8d822f27015c3af02def6ed57b789f47.zip\n",
      "2025-11-06 23:19:26,087 - INFO - File C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc.temp adalah file ZIP\n",
      "2025-11-06 23:19:26,089 - INFO - Mengekstrak file ZIP: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc.temp\n",
      "2025-11-06 23:19:26,092 - INFO - File dalam ZIP: ['data_stream-oper_stepType-instant.nc', 'data_stream-oper_stepType-accum.nc']\n",
      "2025-11-06 23:19:26,100 - INFO - File NetCDF ditemukan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\temp_extract\\data_stream-oper_stepType-instant.nc\n",
      "2025-11-06 23:19:26,108 - INFO - File NetCDF disimpan sebagai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc\n",
      "2025-11-06 23:19:26,112 - INFO - File ZIP asli dihapus: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc.temp\n",
      "2025-11-06 23:19:26,113 - INFO - ‚úÖ Download dan ekstrak selesai: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc\n",
      "2025-11-06 23:19:26,117 - INFO - Total files downloaded: 10\n",
      "2025-11-06 23:19:26,121 - INFO - üîÑ Konversi NetCDF ke CSV (per lokasi)...\n",
      "2025-11-06 23:19:26,136 - INFO - Menemukan 10 file untuk dikonversi:\n",
      "2025-11-06 23:19:26,139 - INFO -    1. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc\n",
      "2025-11-06 23:19:26,142 - INFO -    2. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc\n",
      "2025-11-06 23:19:26,144 - INFO -    3. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc\n",
      "2025-11-06 23:19:26,147 - INFO -    4. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc\n",
      "2025-11-06 23:19:26,150 - INFO -    5. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc\n",
      "2025-11-06 23:19:26,153 - INFO -    6. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc\n",
      "2025-11-06 23:19:26,156 - INFO -    7. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc\n",
      "2025-11-06 23:19:26,158 - INFO -    8. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc\n",
      "2025-11-06 23:19:26,161 - INFO -    9. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc\n",
      "2025-11-06 23:19:26,163 - INFO -   10. C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc\n",
      "2025-11-06 23:19:26,165 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc\n",
      "2025-11-06 23:19:26,170 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:26,195 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_01.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:26,476 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:26,490 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:26,491 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:26,493 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,495 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:26,501 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:26,562 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,590 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:26,592 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:26,596 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:26,612 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,630 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:26,632 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:26,635 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:26,655 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,674 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:26,676 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:26,679 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:26,701 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,718 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:26,722 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:26,727 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:26,747 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:26,767 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,026 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_01_per_location.csv\n",
      "2025-11-06 23:19:27,029 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:27,031 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:27,033 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:27,039 - INFO -    Time range: 2023-01-01 00:00:00 to 2023-01-31 23:00:00\n",
      "2025-11-06 23:19:27,043 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:27,045 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,047 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:27,078 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc\n",
      "2025-11-06 23:19:27,081 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:27,107 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_02.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,150 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,153 - INFO - Dimensi dataset: {'valid_time': 672, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:27,155 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:27,157 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,158 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:27,161 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,180 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,193 - INFO - ‚úÖ Data untuk bundaran_hi: 672 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,195 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:27,197 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,205 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,214 - INFO - ‚úÖ Data untuk kelapa_gading: 672 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,216 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:27,218 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:27,234 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,247 - INFO - ‚úÖ Data untuk jagakarsa: 672 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,248 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:27,249 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,260 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,269 - INFO - ‚úÖ Data untuk lubang_buaya: 672 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,272 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:27,275 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-01-01    3.075758    0.363742               296.163696      297.533478    bundaran_hi    -6.206  106.903999        3.097192            6.744507         24.383484                  23.013702\n",
      "1488 2023-01-01    1.991102   -0.163923               295.289459      296.147369      jagakarsa    -6.457  106.903999        1.997838          355.293579         22.997375                  22.139465\n",
      "2976 2023-01-01    2.074171    0.687701               296.270691      297.626465    kebun_jeruk    -6.206  106.653000        2.185204           18.343170         24.476471                  23.120697\n",
      "744  2023-01-01    3.075758    0.363742               296.163696      297.533478  kelapa_gading    -6.206  106.903999        3.097192            6.744507         24.383484                  23.013702\n",
      "2232 2023-01-01    3.075758    0.363742               296.163696      297.533478   lubang_buaya    -6.206  106.903999        3.097192            6.744507         24.383484                  23.013702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:27,286 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,298 - INFO - ‚úÖ Data untuk kebun_jeruk: 672 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,423 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_02_per_location.csv\n",
      "2025-11-06 23:19:27,425 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:27,426 - INFO -    Total rows: 3,360\n",
      "2025-11-06 23:19:27,428 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:27,429 - INFO -    Time range: 2023-02-01 00:00:00 to 2023-02-28 23:00:00\n",
      "2025-11-06 23:19:27,431 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:27,432 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,433 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:27,447 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc\n",
      "2025-11-06 23:19:27,448 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:27,464 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_05.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,496 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,499 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:27,501 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:27,504 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,506 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:27,509 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,521 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,533 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,536 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:27,538 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,552 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,563 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,565 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:27,567 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:27,581 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,598 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,601 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:27,605 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,636 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-02-01    2.091321    0.208390               296.850830      298.195862    bundaran_hi    -6.206  106.903999        2.101677            5.690460         25.045868                  23.700836\n",
      "1344 2023-02-01    1.059491    0.646531               295.754852      296.526428      jagakarsa    -6.457  106.903999        1.241178           31.392700         23.376434                  22.604858\n",
      "2688 2023-02-01    1.012463    0.550752               296.784637      298.345642    kebun_jeruk    -6.206  106.653000        1.152566           28.544952         25.195648                  23.634644\n",
      "672  2023-02-01    2.091321    0.208390               296.850830      298.195862  kelapa_gading    -6.206  106.903999        2.101677            5.690460         25.045868                  23.700836\n",
      "2016 2023-02-01    2.091321    0.208390               296.850830      298.195862   lubang_buaya    -6.206  106.903999        2.101677            5.690460         25.045868                  23.700836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:27,657 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,660 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:27,666 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:27,689 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,699 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,797 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_05_per_location.csv\n",
      "2025-11-06 23:19:27,799 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:27,800 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:27,801 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:27,806 - INFO -    Time range: 2023-05-01 00:00:00 to 2023-05-31 23:00:00\n",
      "2025-11-06 23:19:27,808 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:27,809 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,810 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:27,821 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc\n",
      "2025-11-06 23:19:27,824 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:27,836 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_06.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,861 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:27,863 - INFO - Dimensi dataset: {'valid_time': 720, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:27,863 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:27,864 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,865 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:27,866 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,880 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,890 - INFO - ‚úÖ Data untuk bundaran_hi: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,892 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:27,894 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,907 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,922 - INFO - ‚úÖ Data untuk kelapa_gading: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,923 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:27,926 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:27,943 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,950 - INFO - ‚úÖ Data untuk jagakarsa: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,952 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:27,953 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:27,971 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:27,981 - INFO - ‚úÖ Data untuk lubang_buaya: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:27,982 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:27,984 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:27,996 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,005 - INFO - ‚úÖ Data untuk kebun_jeruk: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-05-01   -0.047326    0.688315               297.641785      298.680115    bundaran_hi    -6.206  106.903999        0.689940           93.933289         25.530121                  24.491791\n",
      "1488 2023-05-01    0.206466    0.898459               296.583588      297.023651      jagakarsa    -6.457  106.903999        0.921877           77.058136         23.873657                  23.433594\n",
      "2976 2023-05-01   -0.003518    0.848532               297.744171      298.724304    kebun_jeruk    -6.206  106.653000        0.848539           90.237549         25.574310                  24.594177\n",
      "744  2023-05-01   -0.047326    0.688315               297.641785      298.680115  kelapa_gading    -6.206  106.903999        0.689940           93.933289         25.530121                  24.491791\n",
      "2232 2023-05-01   -0.047326    0.688315               297.641785      298.680115   lubang_buaya    -6.206  106.903999        0.689940           93.933289         25.530121                  24.491791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:28,104 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_06_per_location.csv\n",
      "2025-11-06 23:19:28,107 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:28,108 - INFO -    Total rows: 3,600\n",
      "2025-11-06 23:19:28,111 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:28,114 - INFO -    Time range: 2023-06-01 00:00:00 to 2023-06-30 23:00:00\n",
      "2025-11-06 23:19:28,117 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:28,119 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,122 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:28,142 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc\n",
      "2025-11-06 23:19:28,144 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:28,165 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_07.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,208 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,212 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:28,215 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:28,216 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,218 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:28,221 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,232 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,239 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,241 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:28,244 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,257 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,269 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,270 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:28,272 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:28,287 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,299 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,301 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:28,304 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,313 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,323 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,325 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:28,328 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:28,338 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-06-01   -0.381478    1.445623               296.112213      298.812134    bundaran_hi    -6.206  106.903999        1.495110          104.782501         25.662140                  22.962219\n",
      "1440 2023-06-01    0.025749    1.441328               294.633972      296.503723      jagakarsa    -6.457  106.903999        1.441558           88.976562         23.353729                  21.483978\n",
      "2880 2023-06-01   -0.218873    1.386560               296.223663      298.519104    kebun_jeruk    -6.206  106.653000        1.403729           98.970276         25.369110                  23.073669\n",
      "720  2023-06-01   -0.381478    1.445623               296.112213      298.812134  kelapa_gading    -6.206  106.903999        1.495110          104.782501         25.662140                  22.962219\n",
      "2160 2023-06-01   -0.381478    1.445623               296.112213      298.812134   lubang_buaya    -6.206  106.903999        1.495110          104.782501         25.662140                  22.962219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:28,344 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,430 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_07_per_location.csv\n",
      "2025-11-06 23:19:28,431 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:28,432 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:28,434 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:28,435 - INFO -    Time range: 2023-07-01 00:00:00 to 2023-07-31 23:00:00\n",
      "2025-11-06 23:19:28,438 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:28,439 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,440 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:28,447 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc\n",
      "2025-11-06 23:19:28,449 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:28,465 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_08.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,490 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,493 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:28,495 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:28,496 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,497 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:28,500 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,516 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,526 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,528 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:28,530 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,542 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,556 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,556 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:28,559 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:28,574 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,589 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,591 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:28,595 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,604 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,611 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,613 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:28,614 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:28,625 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,633 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-07-01    0.192395    0.970269               296.788330      299.159546    bundaran_hi    -6.206  106.903999        0.989160           78.784302         26.009552                  23.638336\n",
      "1488 2023-07-01    0.345147    1.163254               295.924835      297.001221      jagakarsa    -6.457  106.903999        1.213378           73.473938         23.851227                  22.774841\n",
      "2976 2023-07-01    0.359372    1.011723               297.004822      298.800781    kebun_jeruk    -6.206  106.653000        1.073654           70.444580         25.650787                  23.854828\n",
      "744  2023-07-01    0.192395    0.970269               296.788330      299.159546  kelapa_gading    -6.206  106.903999        0.989160           78.784302         26.009552                  23.638336\n",
      "2232 2023-07-01    0.192395    0.970269               296.788330      299.159546   lubang_buaya    -6.206  106.903999        0.989160           78.784302         26.009552                  23.638336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:28,699 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_08_per_location.csv\n",
      "2025-11-06 23:19:28,701 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:28,703 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:28,704 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:28,706 - INFO -    Time range: 2023-08-01 00:00:00 to 2023-08-31 23:00:00\n",
      "2025-11-06 23:19:28,708 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:28,709 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,711 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:28,718 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc\n",
      "2025-11-06 23:19:28,721 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:28,737 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_09.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,767 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:28,769 - INFO - Dimensi dataset: {'valid_time': 720, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:28,770 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:28,771 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,772 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:28,773 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,790 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,801 - INFO - ‚úÖ Data untuk bundaran_hi: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,803 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:28,805 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,819 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,829 - INFO - ‚úÖ Data untuk kelapa_gading: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,830 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:28,832 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:28,845 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,855 - INFO - ‚úÖ Data untuk jagakarsa: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,858 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:28,861 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:28,880 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,890 - INFO - ‚úÖ Data untuk lubang_buaya: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:28,891 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:28,894 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:28,902 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:28,910 - INFO - ‚úÖ Data untuk kebun_jeruk: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-08-01   -0.974089    1.624541               293.660248      297.564636    bundaran_hi    -6.206  106.903999        1.894197          120.947266         24.414642                  20.510254\n",
      "1488 2023-08-01   -0.029326    1.511008               292.360748      295.029541      jagakarsa    -6.457  106.903999        1.511293           91.111877         21.879547                  19.210754\n",
      "2976 2023-08-01   -0.648436    1.508925               294.194122      297.419922    kebun_jeruk    -6.206  106.653000        1.642354          113.254822         24.269928                  21.044128\n",
      "744  2023-08-01   -0.974089    1.624541               293.660248      297.564636  kelapa_gading    -6.206  106.903999        1.894197          120.947266         24.414642                  20.510254\n",
      "2232 2023-08-01   -0.974089    1.624541               293.660248      297.564636   lubang_buaya    -6.206  106.903999        1.894197          120.947266         24.414642                  20.510254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:28,994 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_09_per_location.csv\n",
      "2025-11-06 23:19:28,996 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:28,998 - INFO -    Total rows: 3,600\n",
      "2025-11-06 23:19:28,999 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:29,002 - INFO -    Time range: 2023-09-01 00:00:00 to 2023-09-30 23:00:00\n",
      "2025-11-06 23:19:29,006 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:29,007 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,008 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:29,018 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc\n",
      "2025-11-06 23:19:29,020 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:29,038 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_10.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,066 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,068 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:29,069 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:29,071 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,072 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:29,074 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,086 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,094 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,095 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:29,097 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,105 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,114 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,116 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:29,117 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:29,125 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,132 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,133 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:29,135 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,144 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,156 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,157 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:29,159 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:29,167 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,176 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-09-01   -0.403240    0.924969               295.054321      297.682251    bundaran_hi    -6.206  106.903999        1.009045          113.554779         24.532257                  21.904327\n",
      "1440 2023-09-01    0.059269    1.299397               292.852539      295.142151      jagakarsa    -6.457  106.903999        1.300748           87.388397         21.992157                  19.702545\n",
      "2880 2023-09-01   -0.121258    1.048833               294.450317      297.257935    kebun_jeruk    -6.206  106.653000        1.055819           96.594818         24.107941                  21.300323\n",
      "720  2023-09-01   -0.403240    0.924969               295.054321      297.682251  kelapa_gading    -6.206  106.903999        1.009045          113.554779         24.532257                  21.904327\n",
      "2160 2023-09-01   -0.403240    0.924969               295.054321      297.682251   lubang_buaya    -6.206  106.903999        1.009045          113.554779         24.532257                  21.904327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:29,270 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_10_per_location.csv\n",
      "2025-11-06 23:19:29,272 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:29,273 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:29,274 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:29,277 - INFO -    Time range: 2023-10-01 00:00:00 to 2023-10-31 23:00:00\n",
      "2025-11-06 23:19:29,280 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:29,282 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,284 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:29,301 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc\n",
      "2025-11-06 23:19:29,302 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:29,330 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_11.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,363 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,365 - INFO - Dimensi dataset: {'valid_time': 720, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:29,367 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:29,368 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,371 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:29,373 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,393 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,409 - INFO - ‚úÖ Data untuk bundaran_hi: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,410 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:29,413 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,432 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,446 - INFO - ‚úÖ Data untuk kelapa_gading: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,447 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:29,450 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n",
      "2025-11-06 23:19:29,470 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,496 - INFO - ‚úÖ Data untuk jagakarsa: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-10-01   -1.237019    0.699842               295.908295      299.254578    bundaran_hi    -6.206  106.903999        1.421265          150.501038         26.104584                  22.758301\n",
      "1488 2023-10-01   -0.630329    0.990385               294.463593      296.853210      jagakarsa    -6.457  106.903999        1.173958          122.474670         23.703217                  21.313599\n",
      "2976 2023-10-01   -0.911442    0.837797               295.552155      299.171753    kebun_jeruk    -6.206  106.653000        1.237994          137.410767         26.021759                  22.402161\n",
      "744  2023-10-01   -1.237019    0.699842               295.908295      299.254578  kelapa_gading    -6.206  106.903999        1.421265          150.501038         26.104584                  22.758301\n",
      "2232 2023-10-01   -1.237019    0.699842               295.908295      299.254578   lubang_buaya    -6.206  106.903999        1.421265          150.501038         26.104584                  22.758301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:29,508 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:29,511 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,521 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,531 - INFO - ‚úÖ Data untuk lubang_buaya: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,532 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:29,534 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:29,544 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,553 - INFO - ‚úÖ Data untuk kebun_jeruk: 720 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,624 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_11_per_location.csv\n",
      "2025-11-06 23:19:29,626 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:29,627 - INFO -    Total rows: 3,600\n",
      "2025-11-06 23:19:29,629 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:29,631 - INFO -    Time range: 2023-11-01 00:00:00 to 2023-11-30 23:00:00\n",
      "2025-11-06 23:19:29,633 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:29,634 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,635 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:29,651 - INFO - Memproses file: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc\n",
      "2025-11-06 23:19:29,655 - INFO - Ukuran file: 0.13 MB\n",
      "2025-11-06 23:19:29,681 - INFO - Mencoba buka C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\raw_data\\data_era5_hourly_2023_12.nc dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,737 - INFO - ‚úÖ Berhasil buka file dengan engine: netcdf4\n",
      "2025-11-06 23:19:29,741 - INFO - Dimensi dataset: {'valid_time': 744, 'latitude': 2, 'longitude': 2}\n",
      "2025-11-06 23:19:29,743 - INFO - Koordinat dataset: ['number', 'valid_time', 'latitude', 'longitude', 'expver']\n",
      "2025-11-06 23:19:29,746 - INFO - Variabel yang tersedia: ['u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,748 - INFO - üìç Memproses lokasi: bundaran_hi\n",
      "2025-11-06 23:19:29,750 - INFO - Target: (-6.1947, 106.8235) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,775 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,792 - INFO - ‚úÖ Data untuk bundaran_hi: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,795 - INFO - üìç Memproses lokasi: kelapa_gading\n",
      "2025-11-06 23:19:29,799 - INFO - Target: (-6.1536, 106.9109) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,816 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,830 - INFO - ‚úÖ Data untuk kelapa_gading: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,833 - INFO - üìç Memproses lokasi: jagakarsa\n",
      "2025-11-06 23:19:29,835 - INFO - Target: (-6.3569, 106.8037) -> Grid: (-6.4570, 106.9040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-11-01   -0.248286    1.182060               297.310944      300.142761    bundaran_hi    -6.206  106.903999        1.207854          101.862244         26.992767                  24.160950\n",
      "1440 2023-11-01    0.194707    0.941246               296.174866      298.400696      jagakarsa    -6.457  106.903999        0.961174           78.312561         25.250702                  23.024872\n",
      "2880 2023-11-01   -0.310740    1.354133               296.770905      300.693665    kebun_jeruk    -6.206  106.653000        1.389330          102.924194         27.543671                  23.620911\n",
      "720  2023-11-01   -0.248286    1.182060               297.310944      300.142761  kelapa_gading    -6.206  106.903999        1.207854          101.862244         26.992767                  24.160950\n",
      "2160 2023-11-01   -0.248286    1.182060               297.310944      300.142761   lubang_buaya    -6.206  106.903999        1.207854          101.862244         26.992767                  24.160950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:29,853 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,874 - INFO - ‚úÖ Data untuk jagakarsa: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,877 - INFO - üìç Memproses lokasi: lubang_buaya\n",
      "2025-11-06 23:19:29,881 - INFO - Target: (-6.2889, 106.9092) -> Grid: (-6.2060, 106.9040)\n",
      "2025-11-06 23:19:29,935 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:29,975 - INFO - ‚úÖ Data untuk lubang_buaya: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:29,977 - INFO - üìç Memproses lokasi: kebun_jeruk\n",
      "2025-11-06 23:19:29,980 - INFO - Target: (-6.2073, 106.7532) -> Grid: (-6.2060, 106.6530)\n",
      "2025-11-06 23:19:29,992 - INFO - Columns in DataFrame: ['valid_time', 'u10', 'v10', 'd2m', 't2m']\n",
      "2025-11-06 23:19:30,000 - INFO - ‚úÖ Data untuk kebun_jeruk: 744 records, columns: ['datetime', 'u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'location_name', 'latitude', 'longitude', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:30,079 - INFO - ‚úÖ File CSV disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_12_per_location.csv\n",
      "2025-11-06 23:19:30,080 - INFO - üìä Data Statistics:\n",
      "2025-11-06 23:19:30,082 - INFO -    Total rows: 3,720\n",
      "2025-11-06 23:19:30,083 - INFO -    Total columns: 12\n",
      "2025-11-06 23:19:30,085 - INFO -    Time range: 2023-12-01 00:00:00 to 2023-12-31 23:00:00\n",
      "2025-11-06 23:19:30,089 - INFO -    Locations: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:30,091 - INFO -    Available variables: ['u_wind_10m', 'v_wind_10m', 'dewpoint_temperature_2m', 'temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'temperature_2m_c', 'dewpoint_temperature_2m_c']\n",
      "2025-11-06 23:19:30,092 - INFO - üëÄ Preview data (first 5 rows):\n",
      "2025-11-06 23:19:30,107 - INFO - \n",
      "==================================================\n",
      "2025-11-06 23:19:30,108 - INFO - SUMMARY KONVERSI:\n",
      "2025-11-06 23:19:30,110 - INFO - ==================================================\n",
      "2025-11-06 23:19:30,112 - INFO - ‚úÖ Berhasil: 10 file\n",
      "2025-11-06 23:19:30,114 - INFO - ‚ùå Gagal: 0 file\n",
      "2025-11-06 23:19:30,116 - INFO - \n",
      "üéâ PIPELINE SELESAI!\n",
      "2025-11-06 23:19:30,118 - INFO - üìÅ File CSV tersimpan di: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\n",
      "2025-11-06 23:19:30,121 - INFO - File yang berhasil dikonversi:\n",
      "2025-11-06 23:19:30,123 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_01_per_location.csv\n",
      "2025-11-06 23:19:30,125 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_02_per_location.csv\n",
      "2025-11-06 23:19:30,128 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_05_per_location.csv\n",
      "2025-11-06 23:19:30,130 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_06_per_location.csv\n",
      "2025-11-06 23:19:30,133 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_07_per_location.csv\n",
      "2025-11-06 23:19:30,137 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_08_per_location.csv\n",
      "2025-11-06 23:19:30,139 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_09_per_location.csv\n",
      "2025-11-06 23:19:30,142 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_10_per_location.csv\n",
      "2025-11-06 23:19:30,144 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_11_per_location.csv\n",
      "2025-11-06 23:19:30,146 - INFO -   üìÑ C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\csv_output\\data_era5_hourly_2023_12_per_location.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       datetime  u_wind_10m  v_wind_10m  dewpoint_temperature_2m  temperature_2m  location_name  latitude   longitude  wind_speed_10m  wind_direction_10m  temperature_2m_c  dewpoint_temperature_2m_c\n",
      "0    2023-12-01    1.230274   -0.545294               297.424927      298.932800    bundaran_hi    -6.206  106.903999        1.345704          336.095642         25.782806                  24.274933\n",
      "1488 2023-12-01    0.622425    0.126825               296.474548      297.080444      jagakarsa    -6.457  106.903999        0.635214           11.516937         23.930450                  23.324554\n",
      "2976 2023-12-01    1.084537   -0.156591               297.516083      298.894165    kebun_jeruk    -6.206  106.653000        1.095783          351.784088         25.744171                  24.366089\n",
      "744  2023-12-01    1.230274   -0.545294               297.424927      298.932800  kelapa_gading    -6.206  106.903999        1.345704          336.095642         25.782806                  24.274933\n",
      "2232 2023-12-01    1.230274   -0.545294               297.424927      298.932800   lubang_buaya    -6.206  106.903999        1.345704          336.095642         25.782806                  24.274933\n"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import zipfile\n",
    "import shutil\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Setup logging untuk monitoring yang lebih baik\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings yang tidak penting\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# KONFIGURASI\n",
    "# =============================================================================\n",
    "\n",
    "class ERA5Config:\n",
    "    \"\"\"Konfigurasi untuk download dan processing data ERA5\"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    CDS_API_KEY = \"0454455d-cef6-4a4e-ad46-f0e905629467\" \n",
    "    CDS_API_URL = \"https://cds.climate.copernicus.eu/api\"\n",
    "    \n",
    "    # Target years dan months\n",
    "    TARGET_YEARS = ['2023']\n",
    "    TARGET_MONTHS = ['01','02','03' '04', '05', '06', '07', '08', '09', '10', '11', '12']  # February dan March\n",
    "    \n",
    "    # Lokasi spesifik yang ingin diambil datanya\n",
    "    LOCATIONS = {\n",
    "        'bundaran_hi': {'lat': -6.19466, 'lon': 106.8235},\n",
    "        'kelapa_gading': {'lat': -6.15358, 'lon': 106.91089},\n",
    "        'jagakarsa': {'lat': -6.35693, 'lon': 106.80367},\n",
    "        'lubang_buaya': {'lat': -6.28889, 'lon': 106.90919},\n",
    "        'kebun_jeruk': {'lat': -6.20735, 'lon': 106.75319}\n",
    "    }\n",
    "    \n",
    "    # Area bounds untuk download (sedikit lebih besar dari area lokasi)\n",
    "    AREA_BOUNDS = [\n",
    "        max(loc['lat'] for loc in LOCATIONS.values()) + 0.1,  # North\n",
    "        min(loc['lon'] for loc in LOCATIONS.values()) - 0.1,  # West  \n",
    "        min(loc['lat'] for loc in LOCATIONS.values()) - 0.1,  # South\n",
    "        max(loc['lon'] for loc in LOCATIONS.values()) + 0.1   # East\n",
    "    ]\n",
    "    \n",
    "    # Variables yang akan didownload\n",
    "    VARIABLES = [\n",
    "        '10m_u_component_of_wind', '10m_v_component_of_wind', \n",
    "        '2m_dewpoint_temperature', '2m_temperature', \n",
    "        'total_precipitation'\n",
    "    ]\n",
    "    \n",
    "    # Path configuration\n",
    "    BASE_PATH = 'C:\\\\Users\\\\user\\\\OneDrive\\\\IPB\\\\Thesis\\\\02. Development\\\\02. Data ERA5\\\\01. Data Praprocessing'\n",
    "    RAW_DATA_PATH = os.path.join(BASE_PATH, 'raw_data')\n",
    "    PROCESSED_DATA_PATH = os.path.join(BASE_PATH, 'processed_data')\n",
    "    CSV_OUTPUT_PATH = os.path.join(BASE_PATH, 'csv_output')\n",
    "    TEMP_EXTRACT_PATH = os.path.join(BASE_PATH, 'temp_extract')\n",
    "\n",
    "# =============================================================================\n",
    "# FUNGSI UTILITY\n",
    "# =============================================================================\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Membuat direktori yang diperlukan\"\"\"\n",
    "    paths = [\n",
    "        ERA5Config.RAW_DATA_PATH,\n",
    "        ERA5Config.PROCESSED_DATA_PATH, \n",
    "        ERA5Config.CSV_OUTPUT_PATH,\n",
    "        ERA5Config.TEMP_EXTRACT_PATH\n",
    "    ]\n",
    "    \n",
    "    for path in paths:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logger.info(f\"Directory created/verified: {path}\")\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Memeriksa semua dependencies yang diperlukan\"\"\"\n",
    "    required_packages = {\n",
    "        'xarray': 'xarray',\n",
    "        'pandas': 'pandas', \n",
    "        'numpy': 'numpy',\n",
    "        'cdsapi': 'cdsapi',\n",
    "        'netcdf4': 'netCDF4'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package, import_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "            logger.info(f\"‚úÖ {package} tersedia\")\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "            logger.error(f\"‚ùå {package} tidak tersedia\")\n",
    "    \n",
    "    if missing_packages:\n",
    "        logger.error(f\"Package yang missing: {missing_packages}\")\n",
    "        logger.error(\"Install dengan: pip install \" + \" \".join(missing_packages))\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def extract_zip_file(zip_path, extract_to=None):\n",
    "    \"\"\"\n",
    "    Extract file ZIP dan cari file NetCDF di dalamnya\n",
    "    \"\"\"\n",
    "    if extract_to is None:\n",
    "        extract_to = ERA5Config.TEMP_EXTRACT_PATH\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Mengekstrak file ZIP: {zip_path}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Dapatkan list file dalam ZIP\n",
    "            file_list = zip_ref.namelist()\n",
    "            logger.info(f\"File dalam ZIP: {file_list}\")\n",
    "            \n",
    "            # Extract semua file\n",
    "            zip_ref.extractall(extract_to)\n",
    "            \n",
    "            # Cari file NetCDF\n",
    "            nc_files = [f for f in file_list if f.endswith('.nc')]\n",
    "            if nc_files:\n",
    "                nc_file_path = os.path.join(extract_to, nc_files[0])\n",
    "                logger.info(f\"File NetCDF ditemukan: {nc_file_path}\")\n",
    "                return nc_file_path\n",
    "            else:\n",
    "                logger.warning(\"Tidak ada file NetCDF dalam ZIP\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting ZIP file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def handle_zip_download(downloaded_file, target_nc_file):\n",
    "    \"\"\"\n",
    "    Handle file yang didownload sebagai ZIP tetapi disimpan sebagai .nc\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cek jika file sebenarnya adalah ZIP\n",
    "        if zipfile.is_zipfile(downloaded_file):\n",
    "            logger.info(f\"File {downloaded_file} adalah file ZIP\")\n",
    "            \n",
    "            # Extract ZIP file\n",
    "            extracted_nc = extract_zip_file(downloaded_file)\n",
    "            \n",
    "            if extracted_nc and os.path.exists(extracted_nc):\n",
    "                # Copy file yang diekstrak ke lokasi target\n",
    "                shutil.copy2(extracted_nc, target_nc_file)\n",
    "                logger.info(f\"File NetCDF disimpan sebagai: {target_nc_file}\")\n",
    "                \n",
    "                # Hapus file ZIP asli\n",
    "                os.remove(downloaded_file)\n",
    "                logger.info(f\"File ZIP asli dihapus: {downloaded_file}\")\n",
    "                \n",
    "                return True\n",
    "            else:\n",
    "                logger.error(\"Gagal mengekstrak file NetCDF dari ZIP\")\n",
    "                return False\n",
    "        else:\n",
    "            logger.info(f\"File {downloaded_file} adalah file NetCDF asli\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error handling ZIP download: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# DOWNLOAD DATA ERA5\n",
    "# =============================================================================\n",
    "\n",
    "class ERA5Downloader:\n",
    "    \"\"\"Class untuk handle download data ERA5\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = cdsapi.Client(\n",
    "            url=ERA5Config.CDS_API_URL, \n",
    "            key=ERA5Config.CDS_API_KEY\n",
    "        )\n",
    "        logger.info(\"CDS API client initialized\")\n",
    "    \n",
    "    def download_monthly_data(self, year, month):\n",
    "        \"\"\"Download data bulanan untuk tahun dan bulan tertentu\"\"\"\n",
    "        \n",
    "        output_file = os.path.join(\n",
    "            ERA5Config.RAW_DATA_PATH, \n",
    "            f'data_era5_hourly_{year}_{month}.nc'\n",
    "        )\n",
    "        \n",
    "        # Skip jika file sudah ada dan ukurannya reasonable\n",
    "        if (os.path.exists(output_file) and \n",
    "            os.path.getsize(output_file) > 10_000_000):  # > 10MB\n",
    "            logger.info(f\"File {output_file} sudah ada, skip download\")\n",
    "            return output_file\n",
    "        \n",
    "        # Prepare request parameters\n",
    "        all_days = [f'{i:02d}' for i in range(1, 32)]\n",
    "        all_hours = [f'{h:02d}:00' for h in range(24)]\n",
    "        \n",
    "        request_params = {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'netcdf',\n",
    "            'variable': ERA5Config.VARIABLES,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': all_days,\n",
    "            'time': all_hours,\n",
    "            'area': ERA5Config.AREA_BOUNDS,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Downloading data untuk {year}-{month}...\")\n",
    "            \n",
    "            # Download ke temporary file dulu\n",
    "            temp_file = output_file + '.temp'\n",
    "            self.client.retrieve(\n",
    "                'reanalysis-era5-single-levels',\n",
    "                request_params,\n",
    "                temp_file\n",
    "            )\n",
    "            \n",
    "            # Handle kemungkinan file ZIP\n",
    "            if handle_zip_download(temp_file, output_file):\n",
    "                logger.info(f\"‚úÖ Download dan ekstrak selesai: {output_file}\")\n",
    "                return output_file\n",
    "            else:\n",
    "                logger.error(f\"‚ùå Gagal processing file untuk {year}-{month}\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Gagal download {year}-{month}: {str(e)}\")\n",
    "            # Hapus file temporary jika ada\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "            return None\n",
    "    \n",
    "    def download_all_data(self):\n",
    "        \"\"\"Download semua data berdasarkan konfigurasi\"\"\"\n",
    "        downloaded_files = []\n",
    "        \n",
    "        for year, month in product(ERA5Config.TARGET_YEARS, ERA5Config.TARGET_MONTHS):\n",
    "            result = self.download_monthly_data(year, month)\n",
    "            if result:\n",
    "                downloaded_files.append(result)\n",
    "        \n",
    "        logger.info(f\"Total files downloaded: {len(downloaded_files)}\")\n",
    "        return downloaded_files\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESSING DATA NETCDF KE CSV (PER LOKASI SPESIFIK)\n",
    "# =============================================================================\n",
    "\n",
    "class ERA5Processor:\n",
    "    \"\"\"Class untuk processing data ERA5 dari NetCDF ke CSV per lokasi spesifik\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def try_open_netcdf(file_path):\n",
    "        \"\"\"Mencoba berbagai engine untuk membuka file NetCDF\"\"\"\n",
    "        engines_to_try = ['netcdf4', 'scipy', 'h5netcdf']\n",
    "        \n",
    "        for engine in engines_to_try:\n",
    "            try:\n",
    "                logger.info(f\"Mencoba buka {file_path} dengan engine: {engine}\")\n",
    "                ds = xr.open_dataset(file_path, engine=engine)\n",
    "                logger.info(f\"‚úÖ Berhasil buka file dengan engine: {engine}\")\n",
    "                return ds\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ùå Engine {engine} gagal: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Jika semua engine gagal, coba tanpa specify engine\n",
    "        try:\n",
    "            logger.info(\"Mencoba buka file tanpa specify engine...\")\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            logger.info(\"‚úÖ Berhasil buka file tanpa specify engine\")\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Semua metode gagal: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def find_nearest_grid_point(ds, target_lat, target_lon):\n",
    "        \"\"\"Mencari titik grid terdekat dengan lokasi target\"\"\"\n",
    "        try:\n",
    "            # Calculate absolute differences\n",
    "            lat_diff = np.abs(ds.latitude.values - target_lat)\n",
    "            lon_diff = np.abs(ds.longitude.values - target_lon)\n",
    "            \n",
    "            # Find indices of minimum differences\n",
    "            lat_idx = np.argmin(lat_diff)\n",
    "            lon_idx = np.argmin(lon_diff)\n",
    "            \n",
    "            # Get actual coordinates\n",
    "            actual_lat = ds.latitude.values[lat_idx]\n",
    "            actual_lon = ds.longitude.values[lon_idx]\n",
    "            \n",
    "            logger.info(f\"Target: ({target_lat:.4f}, {target_lon:.4f}) -> Grid: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            return lat_idx, lon_idx, actual_lat, actual_lon\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding nearest grid point: {str(e)}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_data_for_location(ds, location_name, target_lat, target_lon):\n",
    "        \"\"\"Extract data untuk satu lokasi spesifik\"\"\"\n",
    "        try:\n",
    "            # Find nearest grid point\n",
    "            lat_idx, lon_idx, actual_lat, actual_lon = ERA5Processor.find_nearest_grid_point(\n",
    "                ds, target_lat, target_lon\n",
    "            )\n",
    "            \n",
    "            if lat_idx is None:\n",
    "                return None\n",
    "            \n",
    "            # Extract data untuk titik tersebut\n",
    "            location_data = {}\n",
    "            \n",
    "            for var_name in ds.data_vars:\n",
    "                try:\n",
    "                    # Select data untuk titik lokasi\n",
    "                    var_data = ds[var_name].isel(latitude=lat_idx, longitude=lon_idx)\n",
    "                    \n",
    "                    # Convert to pandas Series\n",
    "                    var_series = var_data.to_series()\n",
    "                    \n",
    "                    # Store in dictionary\n",
    "                    location_data[var_name] = var_series\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error extracting {var_name} for {location_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if not location_data:\n",
    "                logger.error(f\"Tidak ada data yang berhasil diekstrak untuk {location_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(location_data)\n",
    "            df.reset_index(inplace=True)\n",
    "            \n",
    "            # Debug: Print column names untuk troubleshooting\n",
    "            logger.info(f\"Columns in DataFrame: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Cari kolom waktu yang benar\n",
    "            time_column = None\n",
    "            possible_time_columns = ['valid_time', 'time', 'datetime', 'index']\n",
    "            for col in possible_time_columns:\n",
    "                if col in df.columns:\n",
    "                    time_column = col\n",
    "                    break\n",
    "            \n",
    "            if time_column is None:\n",
    "                # Jika tidak ada kolom waktu yang jelas, gunakan index\n",
    "                df['datetime'] = df.index\n",
    "                time_column = 'datetime'\n",
    "            \n",
    "            # Rename kolom waktu ke 'datetime'\n",
    "            if time_column != 'datetime':\n",
    "                df.rename(columns={time_column: 'datetime'}, inplace=True)\n",
    "            \n",
    "            # Add location information\n",
    "            df['location_name'] = location_name\n",
    "            df['latitude'] = actual_lat\n",
    "            df['longitude'] = actual_lon\n",
    "            \n",
    "            # Rename columns untuk lebih jelas\n",
    "            column_mapping = {\n",
    "                't2m': 'temperature_2m',\n",
    "                'd2m': 'dewpoint_temperature_2m',\n",
    "                'u10': 'u_wind_10m',\n",
    "                'v10': 'v_wind_10m',\n",
    "                'tp': 'precipitation'\n",
    "            }\n",
    "            \n",
    "            df.rename(columns=column_mapping, inplace=True)\n",
    "            \n",
    "            # Calculate additional variables\n",
    "            if 'u_wind_10m' in df.columns and 'v_wind_10m' in df.columns:\n",
    "                df['wind_speed_10m'] = np.sqrt(df['u_wind_10m']**2 + df['v_wind_10m']**2)\n",
    "                df['wind_direction_10m'] = np.arctan2(df['v_wind_10m'], df['u_wind_10m']) * 180 / np.pi\n",
    "                df['wind_direction_10m'] = (df['wind_direction_10m'] + 360) % 360  # Convert to 0-360\n",
    "            \n",
    "            # Convert temperature from Kelvin to Celsius\n",
    "            if 'temperature_2m' in df.columns:\n",
    "                df['temperature_2m_c'] = df['temperature_2m'] - 273.15\n",
    "            \n",
    "            if 'dewpoint_temperature_2m' in df.columns:\n",
    "                df['dewpoint_temperature_2m_c'] = df['dewpoint_temperature_2m'] - 273.15\n",
    "            \n",
    "            logger.info(f\"‚úÖ Data untuk {location_name}: {len(df)} records, columns: {df.columns.tolist()}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting data for {location_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_era5_to_csv_per_location(nc_file_path, output_dir=None):\n",
    "        \"\"\"\n",
    "        Konversi file NetCDF ERA5 ke format CSV dengan data per lokasi spesifik\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Memproses file: {nc_file_path}\")\n",
    "            \n",
    "            # Validasi file\n",
    "            if not os.path.exists(nc_file_path):\n",
    "                logger.error(f\"File tidak ditemukan: {nc_file_path}\")\n",
    "                return None\n",
    "            \n",
    "            file_size = os.path.getsize(nc_file_path) / (1024 * 1024)  # MB\n",
    "            logger.info(f\"Ukuran file: {file_size:.2f} MB\")\n",
    "            \n",
    "            # Cek jika file adalah ZIP\n",
    "            if zipfile.is_zipfile(nc_file_path):\n",
    "                logger.info(f\"File {nc_file_path} adalah ZIP file, mengekstrak...\")\n",
    "                extracted_file = extract_zip_file(nc_file_path)\n",
    "                if extracted_file:\n",
    "                    nc_file_path = extracted_file\n",
    "                else:\n",
    "                    logger.error(\"Gagal mengekstrak file ZIP\")\n",
    "                    return None\n",
    "            \n",
    "            # Buka file NetCDF dengan multiple engine fallback\n",
    "            ds = ERA5Processor.try_open_netcdf(nc_file_path)\n",
    "            if ds is None:\n",
    "                return None\n",
    "            \n",
    "            # Ekstrak informasi dataset\n",
    "            logger.info(f\"Dimensi dataset: {dict(ds.dims)}\")\n",
    "            logger.info(f\"Koordinat dataset: {list(ds.coords)}\")\n",
    "            logger.info(f\"Variabel yang tersedia: {list(ds.data_vars)}\")\n",
    "            \n",
    "            # Extract data untuk setiap lokasi\n",
    "            all_location_data = []\n",
    "            \n",
    "            for location_name, coords in ERA5Config.LOCATIONS.items():\n",
    "                logger.info(f\"üìç Memproses lokasi: {location_name}\")\n",
    "                \n",
    "                location_df = ERA5Processor.extract_data_for_location(\n",
    "                    ds, location_name, coords['lat'], coords['lon']\n",
    "                )\n",
    "                \n",
    "                if location_df is not None:\n",
    "                    all_location_data.append(location_df)\n",
    "            \n",
    "            if not all_location_data:\n",
    "                logger.error(\"Tidak ada data yang berhasil diekstrak untuk semua lokasi\")\n",
    "                ds.close()\n",
    "                return None\n",
    "            \n",
    "            # Gabungkan data semua lokasi\n",
    "            combined_df = pd.concat(all_location_data, ignore_index=True)\n",
    "            \n",
    "            # Optimasi tipe data\n",
    "            combined_df = ERA5Processor.optimize_dataframe(combined_df)\n",
    "            \n",
    "            # Urutkan data\n",
    "            if 'datetime' in combined_df.columns:\n",
    "                combined_df.sort_values(['datetime', 'location_name'], inplace=True)\n",
    "            \n",
    "            # Simpan ke CSV\n",
    "            base_name = Path(nc_file_path).stem\n",
    "            if output_dir is None:\n",
    "                output_dir = ERA5Config.CSV_OUTPUT_PATH\n",
    "            \n",
    "            output_file = os.path.join(output_dir, f\"{base_name}_per_location.csv\")\n",
    "            \n",
    "            # Simpan ke CSV\n",
    "            combined_df.to_csv(output_file, index=False)\n",
    "            logger.info(f\"‚úÖ File CSV disimpan: {output_file}\")\n",
    "            \n",
    "            # Log statistics\n",
    "            ERA5Processor.log_data_statistics(combined_df)\n",
    "            \n",
    "            ds.close()\n",
    "            return output_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error processing {nc_file_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def optimize_dataframe(df):\n",
    "        \"\"\"Optimasi tipe data untuk menghemat memory\"\"\"\n",
    "        # Optimasi numeric columns\n",
    "        float_cols = df.select_dtypes(include=['float64']).columns\n",
    "        for col in float_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_data_statistics(df):\n",
    "        \"\"\"Log statistics data\"\"\"\n",
    "        logger.info(\"üìä Data Statistics:\")\n",
    "        logger.info(f\"   Total rows: {len(df):,}\")\n",
    "        logger.info(f\"   Total columns: {len(df.columns)}\")\n",
    "        \n",
    "        if 'datetime' in df.columns:\n",
    "            logger.info(f\"   Time range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "        \n",
    "        logger.info(f\"   Locations: {df['location_name'].unique().tolist()}\")\n",
    "        logger.info(f\"   Available variables: {[col for col in df.columns if col not in ['datetime', 'location_name', 'latitude', 'longitude']]}\")\n",
    "        \n",
    "        # Preview data\n",
    "        logger.info(\"üëÄ Preview data (first 5 rows):\")\n",
    "        print(df.head().to_string())\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_convert_era5_files(input_pattern=None, output_dir=None):\n",
    "        \"\"\"Konversi batch semua file ERA5\"\"\"\n",
    "        \n",
    "        if input_pattern is None:\n",
    "            input_pattern = os.path.join(ERA5Config.RAW_DATA_PATH, \"data_era5_hourly_*.nc\")\n",
    "        \n",
    "        if output_dir is None:\n",
    "            output_dir = ERA5Config.CSV_OUTPUT_PATH\n",
    "        \n",
    "        # Cari file NetCDF\n",
    "        nc_files = glob.glob(input_pattern)\n",
    "        \n",
    "        if not nc_files:\n",
    "            logger.warning(f\"Tidak ada file ditemukan dengan pattern: {input_pattern}\")\n",
    "            \n",
    "            # Coba pattern alternatif\n",
    "            alternative_patterns = [\n",
    "                os.path.join(ERA5Config.RAW_DATA_PATH, \"*.nc\"),\n",
    "                \"data_era5_hourly_*.nc\",\n",
    "                \"era5_*.nc\"\n",
    "            ]\n",
    "            \n",
    "            for pattern in alternative_patterns:\n",
    "                nc_files = glob.glob(pattern)\n",
    "                if nc_files:\n",
    "                    logger.info(f\"Menggunakan pattern alternatif: {pattern}\")\n",
    "                    break\n",
    "        \n",
    "        if not nc_files:\n",
    "            logger.error(\"Tidak ada file NetCDF yang ditemukan!\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Menemukan {len(nc_files)} file untuk dikonversi:\")\n",
    "        for i, nc_file in enumerate(sorted(nc_files), 1):\n",
    "            logger.info(f\"  {i:2d}. {nc_file}\")\n",
    "        \n",
    "        # Process setiap file\n",
    "        converted_files = []\n",
    "        failed_files = []\n",
    "        \n",
    "        for nc_file in sorted(nc_files):\n",
    "            result = ERA5Processor.convert_era5_to_csv_per_location(nc_file, output_dir)\n",
    "            if result:\n",
    "                converted_files.append(result)\n",
    "            else:\n",
    "                failed_files.append(nc_file)\n",
    "        \n",
    "        # Summary\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(\"SUMMARY KONVERSI:\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        logger.info(f\"‚úÖ Berhasil: {len(converted_files)} file\")\n",
    "        logger.info(f\"‚ùå Gagal: {len(failed_files)} file\")\n",
    "        \n",
    "        if failed_files:\n",
    "            logger.info(\"File yang gagal:\")\n",
    "            for ff in failed_files:\n",
    "                logger.info(f\"  - {ff}\")\n",
    "        \n",
    "        return converted_files\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function untuk menjalankan seluruh pipeline\"\"\"\n",
    "    logger.info(\"üöÄ MEMULAI ERA5 DATA PROCESSING PIPELINE\")\n",
    "    \n",
    "    # Step 1: Setup environment\n",
    "    logger.info(\"üîß Setup environment...\")\n",
    "    if not check_dependencies():\n",
    "        logger.error(\"Dependencies tidak lengkap, proses dihentikan\")\n",
    "        return\n",
    "    \n",
    "    setup_directories()\n",
    "    \n",
    "    # Step 2: Download data\n",
    "    logger.info(\"üì• Download data ERA5...\")\n",
    "    downloader = ERA5Downloader()\n",
    "    downloaded_files = downloader.download_all_data()\n",
    "    \n",
    "    if not downloaded_files:\n",
    "        logger.warning(\"Tidak ada file yang didownload, lanjut ke file yang sudah ada\")\n",
    "    \n",
    "    # Step 3: Process data ke CSV per lokasi\n",
    "    logger.info(\"üîÑ Konversi NetCDF ke CSV (per lokasi)...\")\n",
    "    processor = ERA5Processor()\n",
    "    converted_files = processor.batch_convert_era5_files()\n",
    "    \n",
    "    # Final summary\n",
    "    logger.info(f\"\\nüéâ PIPELINE SELESAI!\")\n",
    "    logger.info(f\"üìÅ File CSV tersimpan di: {ERA5Config.CSV_OUTPUT_PATH}\")\n",
    "    \n",
    "    if converted_files:\n",
    "        logger.info(\"File yang berhasil dikonversi:\")\n",
    "        for cf in converted_files:\n",
    "            logger.info(f\"  üìÑ {cf}\")\n",
    "    else:\n",
    "        logger.error(\"‚ùå Tidak ada file yang berhasil dikonversi!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085ec05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 23:19:30,444 - INFO - üöÄ MEMULAI PROCESS GABUNGKAN SEMUA CSV FILES\n",
      "2025-11-06 23:19:30,447 - INFO - ============================================================\n",
      "2025-11-06 23:19:30,459 - INFO - Menemukan 10 file CSV:\n",
      "2025-11-06 23:19:30,463 - INFO -    1. data_era5_hourly_2023_01_per_location.csv (462.1 KB)\n",
      "2025-11-06 23:19:30,465 - INFO -    2. data_era5_hourly_2023_02_per_location.csv (417.2 KB)\n",
      "2025-11-06 23:19:30,467 - INFO -    3. data_era5_hourly_2023_05_per_location.csv (467.5 KB)\n",
      "2025-11-06 23:19:30,471 - INFO -    4. data_era5_hourly_2023_06_per_location.csv (452.4 KB)\n",
      "2025-11-06 23:19:30,474 - INFO -    5. data_era5_hourly_2023_07_per_location.csv (467.1 KB)\n",
      "2025-11-06 23:19:30,477 - INFO -    6. data_era5_hourly_2023_08_per_location.csv (467.4 KB)\n",
      "2025-11-06 23:19:30,479 - INFO -    7. data_era5_hourly_2023_09_per_location.csv (452.0 KB)\n",
      "2025-11-06 23:19:30,481 - INFO -    8. data_era5_hourly_2023_10_per_location.csv (467.4 KB)\n",
      "2025-11-06 23:19:30,483 - INFO -    9. data_era5_hourly_2023_11_per_location.csv (452.3 KB)\n",
      "2025-11-06 23:19:30,486 - INFO -   10. data_era5_hourly_2023_12_per_location.csv (465.4 KB)\n",
      "2025-11-06 23:19:30,490 - INFO - Membaca file: data_era5_hourly_2023_01_per_location.csv\n",
      "2025-11-06 23:19:30,731 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:30,733 - INFO - Membaca file: data_era5_hourly_2023_02_per_location.csv\n",
      "2025-11-06 23:19:30,814 - INFO -   ‚úÖ Berhasil membaca 3360 rows\n",
      "2025-11-06 23:19:30,817 - INFO - Membaca file: data_era5_hourly_2023_05_per_location.csv\n",
      "2025-11-06 23:19:30,894 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:30,895 - INFO - Membaca file: data_era5_hourly_2023_06_per_location.csv\n",
      "2025-11-06 23:19:30,975 - INFO -   ‚úÖ Berhasil membaca 3600 rows\n",
      "2025-11-06 23:19:30,977 - INFO - Membaca file: data_era5_hourly_2023_07_per_location.csv\n",
      "2025-11-06 23:19:31,081 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:31,083 - INFO - Membaca file: data_era5_hourly_2023_08_per_location.csv\n",
      "2025-11-06 23:19:31,167 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:31,171 - INFO - Membaca file: data_era5_hourly_2023_09_per_location.csv\n",
      "2025-11-06 23:19:31,264 - INFO -   ‚úÖ Berhasil membaca 3600 rows\n",
      "2025-11-06 23:19:31,268 - INFO - Membaca file: data_era5_hourly_2023_10_per_location.csv\n",
      "2025-11-06 23:19:31,361 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:31,363 - INFO - Membaca file: data_era5_hourly_2023_11_per_location.csv\n",
      "2025-11-06 23:19:31,456 - INFO -   ‚úÖ Berhasil membaca 3600 rows\n",
      "2025-11-06 23:19:31,460 - INFO - Membaca file: data_era5_hourly_2023_12_per_location.csv\n",
      "2025-11-06 23:19:31,520 - INFO -   ‚úÖ Berhasil membaca 3720 rows\n",
      "2025-11-06 23:19:31,533 - INFO - üìä Total data setelah digabung: 36,480 rows\n",
      "2025-11-06 23:19:31,539 - INFO - üìç Lokasi: ['bundaran_hi', 'jagakarsa', 'kebun_jeruk', 'kelapa_gading', 'lubang_buaya']\n",
      "2025-11-06 23:19:31,541 - INFO - üìÖ Rentang waktu: 2023-01-01 00:00:00 hingga 2023-12-31 23:00:00\n",
      "2025-11-06 23:19:31,544 - INFO - Membuat dataset hourly...\n",
      "2025-11-06 23:19:31,587 - INFO - ‚úÖ Dataset hourly: 36,480 records\n",
      "2025-11-06 23:19:31,589 - INFO - üìã Kolom hourly: ['datetime', 'location_name', 'latitude', 'longitude', 'temperature_2m_c', 'dewpoint_temperature_2m_c', 'u_wind_10m', 'v_wind_10m', 'wind_speed_10m', 'wind_direction_10m']\n",
      "2025-11-06 23:19:31,591 - INFO - Mengkonversi data hourly ke daily...\n",
      "2025-11-06 23:19:34,297 - INFO - ‚úÖ Dataset daily: 1,520 records\n",
      "2025-11-06 23:19:34,299 - INFO - üìã Kolom daily: ['date', 'location_name', 'latitude', 'longitude', 'hourly_observations_count', 'temperature_2m_c_daily_mean', 'temperature_2m_c_daily_min', 'temperature_2m_c_daily_max', 'dewpoint_temperature_2m_c_daily_mean', 'dewpoint_temperature_2m_c_daily_min', 'dewpoint_temperature_2m_c_daily_max', 'wind_speed_10m_daily_mean', 'wind_speed_10m_daily_max', 'u_wind_10m_daily_mean', 'v_wind_10m_daily_mean', 'wind_direction_10m_daily_mean', 'temperature_2m_c_daily_range', 'resultant_wind_speed']\n",
      "2025-11-06 23:19:34,903 - INFO - ‚úÖ Data hourly disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\combined_output\\era5_combined_hourly.csv\n",
      "2025-11-06 23:19:34,953 - INFO - ‚úÖ Data daily disimpan: C:\\Users\\user\\OneDrive\\IPB\\Thesis\\02. Development\\02. Data ERA5\\01. Data Praprocessing\\combined_output\\era5_combined_daily.csv\n",
      "2025-11-06 23:19:34,957 - WARNING - Tidak bisa menyimpan format Parquet\n",
      "2025-11-06 23:19:34,958 - INFO - \n",
      "============================================================\n",
      "2025-11-06 23:19:34,959 - INFO - üìä COMBINED DATA SUMMARY REPORT\n",
      "2025-11-06 23:19:34,960 - INFO - ============================================================\n",
      "2025-11-06 23:19:34,961 - INFO - HOURLY DATA:\n",
      "2025-11-06 23:19:34,961 - INFO -   ‚Ä¢ Total records: 36,480\n",
      "2025-11-06 23:19:34,965 - INFO -   ‚Ä¢ Date range: 2023-01-01 00:00:00 to 2023-12-31 23:00:00\n",
      "2025-11-06 23:19:34,970 - INFO -   ‚Ä¢ Locations: bundaran_hi, jagakarsa, kebun_jeruk, kelapa_gading, lubang_buaya\n",
      "2025-11-06 23:19:34,972 - INFO -   ‚Ä¢ Variables available: 7\n",
      "2025-11-06 23:19:34,973 - INFO - \n",
      "DAILY DATA:\n",
      "2025-11-06 23:19:34,974 - INFO -   ‚Ä¢ Total records: 1,520\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Categorical is not ordered for operation min\nyou can use .as_ordered() to change the Categorical to an ordered one\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 415\u001b[39m\n\u001b[32m    412\u001b[39m     combiner.run_combination()\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 412\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    409\u001b[39m base_path = \u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mIPB\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mThesis\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m02. Development\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m02. Data ERA5\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m01. Data Praprocessing\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    411\u001b[39m combiner = CSVCombiner(base_path)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[43mcombiner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_combination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 400\u001b[39m, in \u001b[36mCSVCombiner.run_combination\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    397\u001b[39m hourly_file, daily_file = \u001b[38;5;28mself\u001b[39m.save_combined_data(hourly_df, daily_df)\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# Step 6: Generate report\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_summary_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhourly_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaily_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ PROCESS SELESAI!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÅ File output tersimpan di: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.COMBINED_OUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 341\u001b[39m, in \u001b[36mCSVCombiner.generate_summary_report\u001b[39m\u001b[34m(self, hourly_df, daily_df)\u001b[39m\n\u001b[32m    339\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDAILY DATA:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Total records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(daily_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Date range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdaily_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_df[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    342\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Locations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(daily_df[\u001b[33m'\u001b[39m\u001b[33mlocation_name\u001b[39m\u001b[33m'\u001b[39m].unique())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    343\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Ä¢ Average observations per day: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_df[\u001b[33m'\u001b[39m\u001b[33mhourly_observations_count\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:6528\u001b[39m, in \u001b[36mSeries.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m   6520\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m1\u001b[39m))\n\u001b[32m   6521\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m   6522\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6526\u001b[39m     **kwargs,\n\u001b[32m   6527\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m6528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:12453\u001b[39m, in \u001b[36mNDFrame.min\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12446\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmin\u001b[39m(\n\u001b[32m  12447\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12448\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12451\u001b[39m     **kwargs,\n\u001b[32m  12452\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m12453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12454\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  12455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12456\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12459\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  12460\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:6464\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   6460\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_axis_number(axis)\n\u001b[32m   6462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delegate, ExtensionArray):\n\u001b[32m   6463\u001b[39m     \u001b[38;5;66;03m# dispatch to ExtensionArray interface\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdelegate\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6467\u001b[39m     \u001b[38;5;66;03m# dispatch to numpy arrays\u001b[39;00m\n\u001b[32m   6468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m numeric_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype.kind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miufcb\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   6469\u001b[39m         \u001b[38;5;66;03m# i.e. not is_numeric_dtype(self.dtype)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2391\u001b[39m, in \u001b[36mCategorical._reduce\u001b[39m\u001b[34m(self, name, skipna, keepdims, **kwargs)\u001b[39m\n\u001b[32m   2388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reduce\u001b[39m(\n\u001b[32m   2389\u001b[39m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, *, skipna: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, keepdims: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m   2390\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2391\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2392\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33margmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33margmin\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   2393\u001b[39m         \u001b[38;5;66;03m# don't wrap in Categorical!\u001b[39;00m\n\u001b[32m   2394\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\base.py:1958\u001b[39m, in \u001b[36mExtensionArray._reduce\u001b[39m\u001b[34m(self, name, skipna, keepdims, **kwargs)\u001b[39m\n\u001b[32m   1953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1954\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1955\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1956\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoes not support reduction \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1957\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1958\u001b[39m result = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n\u001b[32m   1960\u001b[39m     result = np.array([result])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2417\u001b[39m, in \u001b[36mCategorical.min\u001b[39m\u001b[34m(self, skipna, **kwargs)\u001b[39m\n\u001b[32m   2415\u001b[39m nv.validate_minmax_axis(kwargs.get(\u001b[33m\"\u001b[39m\u001b[33maxis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m   2416\u001b[39m nv.validate_min((), kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2417\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_for_ordered\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._codes):\n\u001b[32m   2420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype.na_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:1901\u001b[39m, in \u001b[36mCategorical.check_for_ordered\u001b[39m\u001b[34m(self, op)\u001b[39m\n\u001b[32m   1899\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"assert that we are ordered\"\"\"\u001b[39;00m\n\u001b[32m   1900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ordered:\n\u001b[32m-> \u001b[39m\u001b[32m1901\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1902\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCategorical is not ordered for operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1903\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use .as_ordered() to change the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1904\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCategorical to an ordered one\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1905\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Categorical is not ordered for operation min\nyou can use .as_ordered() to change the Categorical to an ordered one\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CSVCombiner:\n",
    "    \"\"\"Class untuk menggabungkan semua file CSV menjadi satu file daily dan hourly\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        self.BASE_PATH = base_path\n",
    "        self.CSV_OUTPUT_PATH = os.path.join(base_path, 'csv_output')\n",
    "        self.COMBINED_OUTPUT_PATH = os.path.join(base_path, 'combined_output')\n",
    "        \n",
    "        # Buat direktori jika belum ada\n",
    "        os.makedirs(self.COMBINED_OUTPUT_PATH, exist_ok=True)\n",
    "        \n",
    "        # Daftar variabel yang akan diaggregasi\n",
    "        self.DAILY_AGGREGATIONS = {\n",
    "            # Temperature\n",
    "            'temperature_2m_c': ['mean', 'min', 'max'],\n",
    "            'temperature_2m_k': ['mean', 'min', 'max'],\n",
    "            \n",
    "            # Dewpoint\n",
    "            'dewpoint_temperature_2m_c': ['mean', 'min', 'max'],\n",
    "            'dewpoint_temperature_2m_k': ['mean', 'min', 'max'],\n",
    "            \n",
    "            # Wind\n",
    "            'wind_speed_10m': ['mean', 'max'],\n",
    "            'u_wind_10m': ['mean'],\n",
    "            'v_wind_10m': ['mean'],\n",
    "            'wind_direction_10m': ['mean'],\n",
    "            \n",
    "            # Humidity\n",
    "            'relative_humidity': ['mean', 'min', 'max'],\n",
    "            \n",
    "            # Precipitation\n",
    "            'precipitation_mm': ['sum', 'max'],\n",
    "            'total_precipitation': ['sum', 'max'],\n",
    "            \n",
    "            # Pressure\n",
    "            'surface_pressure': ['mean', 'min', 'max'],\n",
    "            'mean_sea_level_pressure': ['mean', 'min', 'max'],\n",
    "        }\n",
    "    \n",
    "    def find_all_csv_files(self):\n",
    "        \"\"\"Mencari semua file CSV di folder csv_output\"\"\"\n",
    "        pattern = os.path.join(self.CSV_OUTPUT_PATH, \"*.csv\")\n",
    "        csv_files = glob.glob(pattern)\n",
    "        \n",
    "        # Juga cari di subfolder jika ada\n",
    "        pattern_recursive = os.path.join(self.CSV_OUTPUT_PATH, \"**\", \"*.csv\")\n",
    "        csv_files.extend(glob.glob(pattern_recursive, recursive=True))\n",
    "        \n",
    "        # Remove duplicates\n",
    "        csv_files = sorted(list(set(csv_files)))\n",
    "        \n",
    "        logger.info(f\"Menemukan {len(csv_files)} file CSV:\")\n",
    "        for i, file in enumerate(csv_files, 1):\n",
    "            file_size = os.path.getsize(file) / 1024  # KB\n",
    "            logger.info(f\"  {i:2d}. {Path(file).name} ({file_size:.1f} KB)\")\n",
    "        \n",
    "        return csv_files\n",
    "    \n",
    "    def read_and_validate_csv(self, csv_file):\n",
    "        \"\"\"Membaca dan memvalidasi file CSV\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Membaca file: {Path(csv_file).name}\")\n",
    "            \n",
    "            # Baca CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Validasi kolom minimum yang diperlukan\n",
    "            required_cols = ['datetime', 'location_name', 'latitude', 'longitude']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                logger.warning(f\"File {Path(csv_file).name} missing columns: {missing_cols}\")\n",
    "                return None\n",
    "            \n",
    "            # Konversi kolom datetime\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "            \n",
    "            # Hapus rows dengan datetime invalid\n",
    "            original_count = len(df)\n",
    "            df = df.dropna(subset=['datetime'])\n",
    "            if len(df) < original_count:\n",
    "                logger.warning(f\"  Dihapus {original_count - len(df)} rows dengan datetime invalid\")\n",
    "            \n",
    "            # Validasi data numerik\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if col not in ['latitude', 'longitude']:\n",
    "                    # Replace extreme outliers dengan NaN\n",
    "                    q1 = df[col].quantile(0.01)\n",
    "                    q3 = df[col].quantile(0.99)\n",
    "                    iqr = q3 - q1\n",
    "                    lower_bound = q1 - 1.5 * iqr\n",
    "                    upper_bound = q3 + 1.5 * iqr\n",
    "                    \n",
    "                    outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "                    if outliers > 0:\n",
    "                        logger.info(f\"  {outliers} outliers ditemukan di {col}\")\n",
    "                        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "            \n",
    "            logger.info(f\"  ‚úÖ Berhasil membaca {len(df)} rows\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Gagal membaca {Path(csv_file).name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def combine_all_csv_files(self, csv_files):\n",
    "        \"\"\"Menggabungkan semua file CSV menjadi satu DataFrame\"\"\"\n",
    "        all_dataframes = []\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            df = self.read_and_validate_csv(csv_file)\n",
    "            if df is not None:\n",
    "                # Tambahkan source file info\n",
    "                df['source_file'] = Path(csv_file).name\n",
    "                all_dataframes.append(df)\n",
    "        \n",
    "        if not all_dataframes:\n",
    "            logger.error(\"Tidak ada data yang berhasil dibaca!\")\n",
    "            return None\n",
    "        \n",
    "        # Gabungkan semua dataframe\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Urutkan data\n",
    "        combined_df.sort_values(['datetime', 'location_name'], inplace=True)\n",
    "        combined_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        logger.info(f\"üìä Total data setelah digabung: {len(combined_df):,} rows\")\n",
    "        logger.info(f\"üìç Lokasi: {combined_df['location_name'].unique().tolist()}\")\n",
    "        logger.info(f\"üìÖ Rentang waktu: {combined_df['datetime'].min()} hingga {combined_df['datetime'].max()}\")\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def create_hourly_data(self, combined_df):\n",
    "        \"\"\"Membuat dataset hourly dari data yang sudah digabung\"\"\"\n",
    "        logger.info(\"Membuat dataset hourly...\")\n",
    "        \n",
    "        # Pilih kolom yang relevan untuk hourly data\n",
    "        hourly_cols = ['datetime', 'location_name', 'latitude', 'longitude']\n",
    "        \n",
    "        # Tambahkan semua variabel meteorologi yang ada\n",
    "        meteo_vars = [\n",
    "            'temperature_2m_c', 'temperature_2m_k',\n",
    "            'dewpoint_temperature_2m_c', 'dewpoint_temperature_2m_k',\n",
    "            'relative_humidity',\n",
    "            'u_wind_10m', 'v_wind_10m', 'wind_speed_10m', 'wind_direction_10m',\n",
    "            'precipitation_mm', 'total_precipitation',\n",
    "            'surface_pressure', 'mean_sea_level_pressure'\n",
    "        ]\n",
    "        \n",
    "        for var in meteo_vars:\n",
    "            if var in combined_df.columns:\n",
    "                hourly_cols.append(var)\n",
    "        \n",
    "        hourly_df = combined_df[hourly_cols].copy()\n",
    "        \n",
    "        # Optimasi memory\n",
    "        hourly_df = self.optimize_dataframe(hourly_df)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Dataset hourly: {len(hourly_df):,} records\")\n",
    "        logger.info(f\"üìã Kolom hourly: {list(hourly_df.columns)}\")\n",
    "        \n",
    "        return hourly_df\n",
    "    \n",
    "    def create_daily_data(self, hourly_df):\n",
    "        \"\"\"Mengkonversi data hourly ke daily aggregates\"\"\"\n",
    "        logger.info(\"Mengkonversi data hourly ke daily...\")\n",
    "        \n",
    "        # Extract date untuk grouping\n",
    "        hourly_df['date'] = hourly_df['datetime'].dt.date\n",
    "        \n",
    "        daily_data = []\n",
    "        \n",
    "        # Group by date dan location\n",
    "        for (date, location_name), group in hourly_df.groupby(['date', 'location_name']):\n",
    "            if len(group) < 18:  # Skip jika kurang dari 18 observasi per hari\n",
    "                continue\n",
    "                \n",
    "            daily_record = {\n",
    "                'date': date,\n",
    "                'location_name': location_name,\n",
    "                'latitude': group['latitude'].iloc[0],\n",
    "                'longitude': group['longitude'].iloc[0],\n",
    "                'hourly_observations_count': len(group)\n",
    "            }\n",
    "            \n",
    "            # Aggregasi untuk setiap variabel\n",
    "            for var_base, aggs in self.DAILY_AGGREGATIONS.items():\n",
    "                # Cek variasi nama kolom\n",
    "                possible_names = [var_base]\n",
    "                if var_base.endswith('_c'):\n",
    "                    possible_names.append(var_base.replace('_c', '_k'))\n",
    "                \n",
    "                for var_name in possible_names:\n",
    "                    if var_name in group.columns:\n",
    "                        for agg_func in aggs:\n",
    "                            col_name = f\"{var_base}_daily_{agg_func}\"\n",
    "                            \n",
    "                            try:\n",
    "                                if agg_func == 'mean':\n",
    "                                    daily_record[col_name] = group[var_name].mean()\n",
    "                                elif agg_func == 'min':\n",
    "                                    daily_record[col_name] = group[var_name].min()\n",
    "                                elif agg_func == 'max':\n",
    "                                    daily_record[col_name] = group[var_name].max()\n",
    "                                elif agg_func == 'sum':\n",
    "                                    daily_record[col_name] = group[var_name].sum()\n",
    "                            except:\n",
    "                                daily_record[col_name] = np.nan\n",
    "                        break\n",
    "            \n",
    "            # Hitung additional metrics\n",
    "            self.calculate_additional_daily_metrics(daily_record, group)\n",
    "            \n",
    "            daily_data.append(daily_record)\n",
    "        \n",
    "        daily_df = pd.DataFrame(daily_data)\n",
    "        \n",
    "        # Urutkan data\n",
    "        daily_df.sort_values(['date', 'location_name'], inplace=True)\n",
    "        daily_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Optimasi memory\n",
    "        daily_df = self.optimize_dataframe(daily_df)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Dataset daily: {len(daily_df):,} records\")\n",
    "        logger.info(f\"üìã Kolom daily: {list(daily_df.columns)}\")\n",
    "        \n",
    "        return daily_df\n",
    "    \n",
    "    def calculate_additional_daily_metrics(self, daily_record, hourly_group):\n",
    "        \"\"\"Hitung metrik tambahan untuk data daily\"\"\"\n",
    "        \n",
    "        # Daily temperature range\n",
    "        if 'temperature_2m_c_daily_max' in daily_record and 'temperature_2m_c_daily_min' in daily_record:\n",
    "            daily_record['temperature_2m_c_daily_range'] = (\n",
    "                daily_record['temperature_2m_c_daily_max'] - daily_record['temperature_2m_c_daily_min']\n",
    "            )\n",
    "        \n",
    "        # Rainy day flag\n",
    "        if 'precipitation_mm_daily_sum' in daily_record:\n",
    "            daily_record['is_rainy_day'] = daily_record['precipitation_mm_daily_sum'] > 0.1\n",
    "        \n",
    "        # Wind dominance\n",
    "        if 'u_wind_10m_daily_mean' in daily_record and 'v_wind_10m_daily_mean' in daily_record:\n",
    "            u_mean = daily_record['u_wind_10m_daily_mean']\n",
    "            v_mean = daily_record['v_wind_10m_daily_mean']\n",
    "            daily_record['resultant_wind_speed'] = np.sqrt(u_mean**2 + v_mean**2)\n",
    "    \n",
    "    def optimize_dataframe(self, df):\n",
    "        \"\"\"Optimasi tipe data untuk menghemat memory\"\"\"\n",
    "        \n",
    "        # Optimasi numeric columns\n",
    "        float_cols = df.select_dtypes(include=['float64']).columns\n",
    "        for col in float_cols:\n",
    "            if col in df.columns:\n",
    "                # Cek range data untuk memilih tipe yang tepat\n",
    "                if df[col].notna().any():\n",
    "                    col_min = df[col].min()\n",
    "                    col_max = df[col].max()\n",
    "                    \n",
    "                    if col_min >= -32768 and col_max <= 32767:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        # Optimasi string columns\n",
    "        string_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in string_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Convert date columns\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_combined_data(self, hourly_df, daily_df):\n",
    "        \"\"\"Menyimpan data combined ke file CSV\"\"\"\n",
    "        try:\n",
    "            # File paths\n",
    "            hourly_output = os.path.join(self.COMBINED_OUTPUT_PATH, \"era5_combined_hourly.csv\")\n",
    "            daily_output = os.path.join(self.COMBINED_OUTPUT_PATH, \"era5_combined_daily.csv\")\n",
    "            \n",
    "            # Simpan hourly data\n",
    "            hourly_df.to_csv(hourly_output, index=False)\n",
    "            logger.info(f\"‚úÖ Data hourly disimpan: {hourly_output}\")\n",
    "            \n",
    "            # Simpan daily data\n",
    "            daily_df.to_csv(daily_output, index=False)\n",
    "            logger.info(f\"‚úÖ Data daily disimpan: {daily_output}\")\n",
    "            \n",
    "            # Simpan juga sebagai Parquet untuk efisiensi\n",
    "            try:\n",
    "                hourly_parquet = os.path.join(self.COMBINED_OUTPUT_PATH, \"era5_combined_hourly.parquet\")\n",
    "                daily_parquet = os.path.join(self.COMBINED_OUTPUT_PATH, \"era5_combined_daily.parquet\")\n",
    "                \n",
    "                hourly_df.to_parquet(hourly_parquet, index=False)\n",
    "                daily_df.to_parquet(daily_parquet, index=False)\n",
    "                \n",
    "                logger.info(f\"‚úÖ Data Parquet disimpan: {hourly_parquet}, {daily_parquet}\")\n",
    "            except:\n",
    "                logger.warning(\"Tidak bisa menyimpan format Parquet\")\n",
    "            \n",
    "            return hourly_output, daily_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving combined data: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def generate_summary_report(self, hourly_df, daily_df):\n",
    "        \"\"\"Generate summary report lengkap\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\"üìä COMBINED DATA SUMMARY REPORT\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Hourly data summary\n",
    "        logger.info(\"HOURLY DATA:\")\n",
    "        logger.info(f\"  ‚Ä¢ Total records: {len(hourly_df):,}\")\n",
    "        logger.info(f\"  ‚Ä¢ Date range: {hourly_df['datetime'].min()} to {hourly_df['datetime'].max()}\")\n",
    "        logger.info(f\"  ‚Ä¢ Locations: {', '.join(hourly_df['location_name'].unique())}\")\n",
    "        logger.info(f\"  ‚Ä¢ Variables available: {len([col for col in hourly_df.columns if col not in ['datetime', 'location_name', 'latitude', 'longitude', 'source_file']])}\")\n",
    "        \n",
    "        # Daily data summary\n",
    "        logger.info(\"\\nDAILY DATA:\")\n",
    "        logger.info(f\"  ‚Ä¢ Total records: {len(daily_df):,}\")\n",
    "        logger.info(f\"  ‚Ä¢ Date range: {daily_df['date'].min()} to {daily_df['date'].max()}\")\n",
    "        logger.info(f\"  ‚Ä¢ Locations: {', '.join(daily_df['location_name'].unique())}\")\n",
    "        logger.info(f\"  ‚Ä¢ Average observations per day: {daily_df['hourly_observations_count'].mean():.1f}\")\n",
    "        \n",
    "        # Data quality metrics\n",
    "        logger.info(\"\\nüìà DATA QUALITY METRICS:\")\n",
    "        \n",
    "        # Hourly data completeness\n",
    "        hourly_vars = [col for col in hourly_df.columns if col not in ['datetime', 'date', 'location_name', 'latitude', 'longitude', 'source_file']]\n",
    "        for var in hourly_vars[:6]:  # Tampilkan 6 variabel pertama\n",
    "            completeness = hourly_df[var].notna().mean() * 100\n",
    "            logger.info(f\"  ‚Ä¢ {var}: {completeness:.1f}% complete\")\n",
    "        \n",
    "        # Daily data statistics\n",
    "        logger.info(\"\\nüìä DAILY STATISTICS (Sample):\")\n",
    "        temp_stats = [\n",
    "            f\"Avg: {daily_df['temperature_2m_c_daily_mean'].mean():.1f}¬∞C\",\n",
    "            f\"Min: {daily_df['temperature_2m_c_daily_min'].min():.1f}¬∞C\", \n",
    "            f\"Max: {daily_df['temperature_2m_c_daily_max'].max():.1f}¬∞C\"\n",
    "        ]\n",
    "        logger.info(f\"  ‚Ä¢ Temperature: {', '.join(temp_stats)}\")\n",
    "        \n",
    "        if 'precipitation_mm_daily_sum' in daily_df.columns:\n",
    "            rain_days = (daily_df['precipitation_mm_daily_sum'] > 0.1).sum()\n",
    "            logger.info(f\"  ‚Ä¢ Rainy days: {rain_days}/{len(daily_df)} ({rain_days/len(daily_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Data preview\n",
    "        logger.info(f\"\\nüëÄ DAILY DATA PREVIEW (first 5 rows):\")\n",
    "        preview_cols = ['date', 'location_name', 'temperature_2m_c_daily_mean', \n",
    "                       'relative_humidity_daily_mean', 'precipitation_mm_daily_sum']\n",
    "        available_preview_cols = [col for col in preview_cols if col in daily_df.columns]\n",
    "        print(daily_df[available_preview_cols].head().to_string(index=False))\n",
    "    \n",
    "    def run_combination(self):\n",
    "        \"\"\"Menjalankan proses penggabungan seluruhnya\"\"\"\n",
    "        logger.info(\"üöÄ MEMULAI PROCESS GABUNGKAN SEMUA CSV FILES\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Cari semua file CSV\n",
    "        csv_files = self.find_all_csv_files()\n",
    "        if not csv_files:\n",
    "            logger.error(\"Tidak ada file CSV yang ditemukan!\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Baca dan gabungkan semua CSV\n",
    "        combined_df = self.combine_all_csv_files(csv_files)\n",
    "        if combined_df is None:\n",
    "            return\n",
    "        \n",
    "        # Step 3: Buat dataset hourly\n",
    "        hourly_df = self.create_hourly_data(combined_df)\n",
    "        \n",
    "        # Step 4: Buat dataset daily\n",
    "        daily_df = self.create_daily_data(hourly_df)\n",
    "        \n",
    "        # Step 5: Simpan data\n",
    "        hourly_file, daily_file = self.save_combined_data(hourly_df, daily_df)\n",
    "        \n",
    "        # Step 6: Generate report\n",
    "        self.generate_summary_report(hourly_df, daily_df)\n",
    "        \n",
    "        logger.info(f\"\\nüéâ PROCESS SELESAI!\")\n",
    "        logger.info(f\"üìÅ File output tersimpan di: {self.COMBINED_OUTPUT_PATH}\")\n",
    "        logger.info(f\"üìÑ File hourly: {hourly_file}\")\n",
    "        logger.info(f\"üìÑ File daily: {daily_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    base_path = 'C:\\\\Users\\\\user\\\\OneDrive\\\\IPB\\\\Thesis\\\\02. Development\\\\02. Data ERA5\\\\01. Data Praprocessing'\n",
    "    \n",
    "    combiner = CSVCombiner(base_path)\n",
    "    combiner.run_combination()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0125ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
